{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c34f943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import googleapiclient.discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "edb6fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"C:/Users/fabia/OneDrive - Otto-Friedrich-Universität Bamberg/Master\\Masterarbeit/Data/Video/Labeled/\"\n",
    "\n",
    "jul_20 = pd.read_csv(work_dir + \"Juli_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "aug_20 = pd.read_csv(work_dir + \"Aug_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "sep_20 = pd.read_csv(work_dir + \"Sep_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "oct_20 = pd.read_csv(work_dir + \"Oct_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "nov_20 = pd.read_csv(work_dir + \"Nov_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "dec_20 = pd.read_csv(work_dir + \"Dec_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jan_21 = pd.read_csv(work_dir + \"Jan_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "feb_21 = pd.read_csv(work_dir + \"Feb_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "mar_21 = pd.read_csv(work_dir + \"March_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "apr_21 = pd.read_csv(work_dir + \"Apr_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "mai_21 = pd.read_csv(work_dir + \"Mai_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jun_21 = pd.read_csv(work_dir + \"June_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jul_21 = pd.read_csv(work_dir + \"Juli_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "aug_21 = pd.read_csv(work_dir + \"Aug_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "sep_21 = pd.read_csv(work_dir + \"September_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "oct_21 = pd.read_csv(work_dir + \"October_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "nov_21 = pd.read_csv(work_dir + \"Nov_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "dec_21 = pd.read_csv(work_dir + \"December_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jan_22 =pd.read_csv(work_dir + \"January_22-Total_with_information_labeled.csv\", index_col=[0])\n",
    "feb_22 = pd.read_csv(work_dir + \"February_22-Total_with_information_labeled.csv\", index_col=[0])\n",
    "\n",
    "jul_20[\"month\"] = \"July\"\n",
    "aug_20[\"month\"] = \"August\"\n",
    "sep_20[\"month\"] = \"September\"\n",
    "oct_20[\"month\"] = \"October\"\n",
    "nov_20[\"month\"] = \"November\"\n",
    "dec_20[\"month\"] = \"December\"\n",
    "jan_21[\"month\"] = \"January\"\n",
    "feb_21[\"month\"] = \"February\"\n",
    "mar_21[\"month\"] = \"March\"\n",
    "apr_21[\"month\"] = \"April\"\n",
    "mai_21[\"month\"] = \"Mai\"\n",
    "jun_21[\"month\"] = \"June\"\n",
    "jul_21[\"month\"] = \"July\"\n",
    "aug_21[\"month\"] = \"August\"\n",
    "sep_21[\"month\"] = \"September\"\n",
    "oct_21[\"month\"] = \"October\"\n",
    "nov_21[\"month\"] = \"November\"\n",
    "dec_21[\"month\"] = \"December\"\n",
    "jan_22[\"month\"] = \"January\"\n",
    "feb_22[\"month\"] = \"February\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "8328e800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12253, 11)\n",
      "(3783, 11)\n"
     ]
    }
   ],
   "source": [
    "og_merged_df = pd.concat([jul_20, aug_20, sep_20, oct_20, nov_20, dec_20, jan_21, feb_21, mar_21, apr_21, mai_21, jun_21, jul_21, aug_21, sep_21, oct_21, nov_21, dec_21, jan_22, feb_22], ignore_index=True)\n",
    "print(og_merged_df.shape)\n",
    "\n",
    "og_merged_df = og_merged_df.loc[og_merged_df['valid'] == \"valid\"]\n",
    "\n",
    "print(og_merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "61522c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          video_id                                              title  \\\n",
      "2      7l3fREmEiuI  Im Audi Etron von Hamburg nach Leipzig in 9 St...   \n",
      "5      cOwoQ5JTxHM  Bloch reagiert auf Tesla Model Y Reichweitente...   \n",
      "11     L60hLi8KZbk  Der große Elektroauto-Schwindel | Bessere Zeit...   \n",
      "14     i3Ovee0gi6Q  Elektroauto kostenlos aufladen mit &Charge - E...   \n",
      "15     GXG_OuYT1W4  Mit dem Elektro Hyundai schneller von Berlin n...   \n",
      "...            ...                                                ...   \n",
      "11204  TTyN4Qbq1Js  Verbrauchsvergleich Tesla Model 3 SR+ (60kWh) ...   \n",
      "11207  2GU5RBXAbmQ  TESLA: Model 3 aus Grünheide? Ein Zeitrahmen &...   \n",
      "11208  Wg5Gpi9ElvU  500 km Test - Vergleich Kosten und Zeit: TΞSLA...   \n",
      "11210  9lQTLtdJZEk        TESLA: Eine Variante des Model Y gestrichen   \n",
      "11212  LSyjq7BcVb4  Volvo C40 Test | 5 Vorteile gegenüber anderen ...   \n",
      "\n",
      "               published_at                channel_id  view_count  like_count  \\\n",
      "2      2020-07-01T09:30:00Z  UCaCaZ-vKtnMG2_FKmEePChQ      5063.0       478.0   \n",
      "5      2020-07-01T14:00:10Z  UCLINPbYQ9sy6qc-TqtBeVnw    211684.0      5571.0   \n",
      "11     2020-07-02T20:43:06Z  UCPH3ZPeqWqRVZ_ef4vOZgSw    482332.0     17206.0   \n",
      "14     2020-07-02T19:00:11Z  UCBc0Mghy-6jhMXs3T7DluMg     11051.0       300.0   \n",
      "15     2020-07-02T09:30:06Z  UCaCaZ-vKtnMG2_FKmEePChQ      8625.0       432.0   \n",
      "...                     ...                       ...         ...         ...   \n",
      "11204  2022-02-20T11:32:10Z  UCQOnq7u47LRBgWXr1Xc-XWQ      4196.0       143.0   \n",
      "11207  2022-02-05T15:59:43Z  UCpPbcfwq0wOpWrC0gBs73tQ     16111.0       858.0   \n",
      "11208  2022-02-06T13:00:11Z  UC0wANY47B2CDa1_9ByPzz_A      5288.0       147.0   \n",
      "11210  2022-02-18T16:00:11Z  UCpPbcfwq0wOpWrC0gBs73tQ     16094.0       749.0   \n",
      "11212  2022-02-22T14:30:05Z  UCDDj2GWklzZ09X7R9OtsX3Q      5237.0       295.0   \n",
      "\n",
      "       comment_count  job  id  valid     month  \n",
      "2                120    0   0  valid      July  \n",
      "5                826    0   0  valid      July  \n",
      "11              4747    0   0  valid      July  \n",
      "14                63    0   0  valid      July  \n",
      "15               127    0   0  valid      July  \n",
      "...              ...  ...  ..    ...       ...  \n",
      "11204             52    0   0  valid  February  \n",
      "11207             51    0   0  valid  February  \n",
      "11208             51    0   0  valid  February  \n",
      "11210             51    0   0  valid  February  \n",
      "11212             51    0   0  valid  February  \n",
      "\n",
      "[3783 rows x 11 columns]\n",
      "(3529, 11)\n"
     ]
    }
   ],
   "source": [
    "print(og_merged_df)\n",
    "# remove entries where channel_id only appeared once in the dataset --> channel holder cannot hold influence\n",
    "unique_removed_df = og_merged_df[og_merged_df.channel_id.duplicated(keep=False)]\n",
    "\n",
    "print(unique_removed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "cb096f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\4122249972.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  grouped_df = unique_removed_df.groupby(['channel_id'])[\"view_count\", \"like_count\", \"comment_count\"].apply(lambda x : x.astype(int).sum())\n"
     ]
    }
   ],
   "source": [
    "grouped_df = unique_removed_df.groupby(['channel_id'])[\"view_count\", \"like_count\", \"comment_count\"].apply(lambda x : x.astype(int).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "59c4c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          view_count  like_count  comment_count\n",
      "channel_id                                                     \n",
      "UC-k-5F2j4CUzuwkx1QLRlpA      459441       17049           4290\n",
      "UC-r2l4o0_R3eRwOHthmGAcA        7776         177            114\n",
      "UC-s__WrmBUAGRQqeZxcFxlQ      633786       21430           3199\n",
      "UC0PJSqEcY5p3NtG9o9A4RWg       47097        1982            541\n",
      "UC0cCZpU5Ioj1FOuIzj7cSsQ       67332        2776            796\n",
      "...                              ...         ...            ...\n",
      "UCyQpfuhftLvrmjxgEzVH78Q       15392          93            220\n",
      "UCyw3E91jaNVEPqirEyhMRRw       42919        1247            948\n",
      "UCzH549YlZhdhIqhtvz7XHmQ     1006081       58558           4809\n",
      "UCzZ0tUcFR7APjPKdxmgkbJA       58353        2461            252\n",
      "UCzyBlIm4NE2ch85fY3Y4cOA     1170023       13715           2464\n",
      "\n",
      "[208 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df)\n",
    "\n",
    "grouped_df.to_csv(\"grouped_df.csv\", index=True)\n",
    "\n",
    "# Getting a list of subscribers is not possible because the YouTube data api only allows you to retrieve that for your own channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "e33d97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids_lst = grouped_df.index.tolist()\n",
    "\n",
    "chunk_lst = np.array_split(channel_ids_lst, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "23c1cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "api_connection = googleapiclient.discovery.build(api_service_name, api_version, developerKey=\"AIzaSyDVseWOuiO1diHvWYZ0-76WM0RKEWo4DgE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "02824661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topicDetails for topic of channel; statistics for channel statistics, snippet for channel name;\n",
    "#channel_ids = [\"UC4t0KuLYBiXqXF27UtjLxPw\", \"UC-k-5F2j4CUzuwkx1QLRlpA\"]\n",
    "def youtube_api(channel_ids_lst):\n",
    "    channel_ids_lst = channel_ids_lst.tolist()\n",
    "    request = api_connection.channels().list(\n",
    "                        part=\"snippet, statistics, topicDetails\", id=channel_ids_lst, maxResults=50, fields=\"items()\")\n",
    "    response = request.execute()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4b1d2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_titles = []\n",
    "channel_descriptions = []\n",
    "channel_subscribers = []\n",
    "channel_topics = []\n",
    "channel_ids = []\n",
    "\n",
    "for chunk in chunk_lst:\n",
    "    response = youtube_api(chunk)\n",
    "    for item in response[\"items\"]:\n",
    "        channel_ids.append(item[\"id\"])\n",
    "        channel_titles.append(item[\"snippet\"][\"title\"])\n",
    "        channel_descriptions.append(item[\"snippet\"][\"description\"])\n",
    "        try:\n",
    "            channel_subscribers.append(item[\"statistics\"][\"subscriberCount\"])\n",
    "        except:\n",
    "            channel_subscribers.append(None)\n",
    "        channel_topics.append(item[\"topicDetails\"][\"topicCategories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c750071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_info_df = pd.DataFrame(\n",
    "    {'channel_id': channel_ids,\n",
    "    'channel_title': channel_titles,\n",
    "     'channel_description': channel_descriptions,\n",
    "     'channel_subscribers': channel_subscribers,\n",
    "     'channel_topic': channel_topics\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "cfcc90d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   channel_id                  channel_title  \\\n",
      "0    UC1-VOKyTJrgLiBeiJqzeIUQ                 JP Performance   \n",
      "1    UC1XrG1M_hw8103zO2x-oivg                        Galileo   \n",
      "2    UC1w6pNGiiLdZgyNpXUnA4Zw                    DER SPIEGEL   \n",
      "3    UC52Vg-Q5oAgGzpl-srvTSwQ                  Silent Energy   \n",
      "4    UC62IIFhchBWQxLPSyUatD-A                DER AKTIONÄR TV   \n",
      "..                        ...                            ...   \n",
      "203  UCsNdsFTTBj5jcRJSWeJ8-Xg          Michael Schmitt B.E.N   \n",
      "204  UCx6OkrMyBqujuyQlwVpYfNw  POLiS - Cluster of Excellence   \n",
      "205  UCwjxya3zbM2yadFag9WlbYg                   GRIP Elektro   \n",
      "206  UCsj96MCtBs8u4vB6l0gVgTg                   Gercollector   \n",
      "207  UCpTkQ0QEm6qPNY28F91Mubw                SP elektrisiert   \n",
      "\n",
      "                                   channel_description channel_subscribers  \\\n",
      "0    GESCHÄFTSFÜHRER: JEAN PIERRE KRAEMER\\n\\nKLÖNNE...             2230000   \n",
      "1    Willkommen auf dem offiziellen Galileo YouTube...             3170000   \n",
      "2    Willkommen auf dem YouTube-Kanal von SPIEGEL T...             1420000   \n",
      "3    Du kannst meinen Tesla Weiterempfehlungscode f...                 451   \n",
      "4    DER AKTIONÄR TV ist die Nummer 1, wenn es daru...              170000   \n",
      "..                                                 ...                 ...   \n",
      "203  Mein Live-Kanal: https://www.youtube.com/chann...               22500   \n",
      "204  Unterwegs am Laptop arbeiten, mit dem Smartpho...                4240   \n",
      "205  Impressum: \\nhttp://www.RTLZWEI.de/impressum\\n...               50500   \n",
      "206  In the social media-based automotive world, Ge...              232000   \n",
      "207  Auf diesem Kanal berichten wir von unseren Erf...                 570   \n",
      "\n",
      "                                         channel_topic  view_count  \\\n",
      "0    [https://en.wikipedia.org/wiki/Vehicle, https:...     2885366   \n",
      "1        [https://en.wikipedia.org/wiki/Entertainment]     1608258   \n",
      "2    [https://en.wikipedia.org/wiki/Society, https:...      375568   \n",
      "3    [https://en.wikipedia.org/wiki/Lifestyle_(soci...       18345   \n",
      "4    [https://en.wikipedia.org/wiki/Society, https:...       49853   \n",
      "..                                                 ...         ...   \n",
      "203  [https://en.wikipedia.org/wiki/Vehicle, https:...     2061045   \n",
      "204            [https://en.wikipedia.org/wiki/Society]       31323   \n",
      "205  [https://en.wikipedia.org/wiki/Lifestyle_(soci...     1858684   \n",
      "206  [https://en.wikipedia.org/wiki/Vehicle, https:...      354675   \n",
      "207  [https://en.wikipedia.org/wiki/Vehicle, https:...       26959   \n",
      "\n",
      "     like_count  comment_count  \n",
      "0         84570           8217  \n",
      "1         31177           5065  \n",
      "2          4681           2679  \n",
      "3           493            225  \n",
      "4           778            145  \n",
      "..          ...            ...  \n",
      "203      122107          29257  \n",
      "204         728            190  \n",
      "205       34968           5525  \n",
      "206       12742           1648  \n",
      "207         982            129  \n",
      "\n",
      "[208 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(channel_info_df, grouped_df, on='channel_id',  how='inner')\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "de8c9b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          video_id                                              title  \\\n",
      "2      7l3fREmEiuI  Im Audi Etron von Hamburg nach Leipzig in 9 St...   \n",
      "5      cOwoQ5JTxHM  Bloch reagiert auf Tesla Model Y Reichweitente...   \n",
      "11     L60hLi8KZbk  Der große Elektroauto-Schwindel | Bessere Zeit...   \n",
      "14     i3Ovee0gi6Q  Elektroauto kostenlos aufladen mit &Charge - E...   \n",
      "15     GXG_OuYT1W4  Mit dem Elektro Hyundai schneller von Berlin n...   \n",
      "...            ...                                                ...   \n",
      "11204  TTyN4Qbq1Js  Verbrauchsvergleich Tesla Model 3 SR+ (60kWh) ...   \n",
      "11207  2GU5RBXAbmQ  TESLA: Model 3 aus Grünheide? Ein Zeitrahmen &...   \n",
      "11208  Wg5Gpi9ElvU  500 km Test - Vergleich Kosten und Zeit: TΞSLA...   \n",
      "11210  9lQTLtdJZEk        TESLA: Eine Variante des Model Y gestrichen   \n",
      "11212  LSyjq7BcVb4  Volvo C40 Test | 5 Vorteile gegenüber anderen ...   \n",
      "\n",
      "               published_at                channel_id  view_count  like_count  \\\n",
      "2      2020-07-01T09:30:00Z  UCaCaZ-vKtnMG2_FKmEePChQ      5063.0       478.0   \n",
      "5      2020-07-01T14:00:10Z  UCLINPbYQ9sy6qc-TqtBeVnw    211684.0      5571.0   \n",
      "11     2020-07-02T20:43:06Z  UCPH3ZPeqWqRVZ_ef4vOZgSw    482332.0     17206.0   \n",
      "14     2020-07-02T19:00:11Z  UCBc0Mghy-6jhMXs3T7DluMg     11051.0       300.0   \n",
      "15     2020-07-02T09:30:06Z  UCaCaZ-vKtnMG2_FKmEePChQ      8625.0       432.0   \n",
      "...                     ...                       ...         ...         ...   \n",
      "11204  2022-02-20T11:32:10Z  UCQOnq7u47LRBgWXr1Xc-XWQ      4196.0       143.0   \n",
      "11207  2022-02-05T15:59:43Z  UCpPbcfwq0wOpWrC0gBs73tQ     16111.0       858.0   \n",
      "11208  2022-02-06T13:00:11Z  UC0wANY47B2CDa1_9ByPzz_A      5288.0       147.0   \n",
      "11210  2022-02-18T16:00:11Z  UCpPbcfwq0wOpWrC0gBs73tQ     16094.0       749.0   \n",
      "11212  2022-02-22T14:30:05Z  UCDDj2GWklzZ09X7R9OtsX3Q      5237.0       295.0   \n",
      "\n",
      "       comment_count  job  id  valid     month  \n",
      "2                120    0   0  valid      July  \n",
      "5                826    0   0  valid      July  \n",
      "11              4747    0   0  valid      July  \n",
      "14                63    0   0  valid      July  \n",
      "15               127    0   0  valid      July  \n",
      "...              ...  ...  ..    ...       ...  \n",
      "11204             52    0   0  valid  February  \n",
      "11207             51    0   0  valid  February  \n",
      "11208             51    0   0  valid  February  \n",
      "11210             51    0   0  valid  February  \n",
      "11212             51    0   0  valid  February  \n",
      "\n",
      "[3783 rows x 11 columns]\n",
      "          video_id                                              title  \\\n",
      "2      7l3fREmEiuI  Im Audi Etron von Hamburg nach Leipzig in 9 St...   \n",
      "5      cOwoQ5JTxHM  Bloch reagiert auf Tesla Model Y Reichweitente...   \n",
      "11     L60hLi8KZbk  Der große Elektroauto-Schwindel | Bessere Zeit...   \n",
      "14     i3Ovee0gi6Q  Elektroauto kostenlos aufladen mit &Charge - E...   \n",
      "15     GXG_OuYT1W4  Mit dem Elektro Hyundai schneller von Berlin n...   \n",
      "...            ...                                                ...   \n",
      "11204  TTyN4Qbq1Js  Verbrauchsvergleich Tesla Model 3 SR+ (60kWh) ...   \n",
      "11207  2GU5RBXAbmQ  TESLA: Model 3 aus Grünheide? Ein Zeitrahmen &...   \n",
      "11208  Wg5Gpi9ElvU  500 km Test - Vergleich Kosten und Zeit: TΞSLA...   \n",
      "11210  9lQTLtdJZEk        TESLA: Eine Variante des Model Y gestrichen   \n",
      "11212  LSyjq7BcVb4  Volvo C40 Test | 5 Vorteile gegenüber anderen ...   \n",
      "\n",
      "               published_at                channel_id  view_count  like_count  \\\n",
      "2      2020-07-01T09:30:00Z  UCaCaZ-vKtnMG2_FKmEePChQ        5063         478   \n",
      "5      2020-07-01T14:00:10Z  UCLINPbYQ9sy6qc-TqtBeVnw      211684        5571   \n",
      "11     2020-07-02T20:43:06Z  UCPH3ZPeqWqRVZ_ef4vOZgSw      482332       17206   \n",
      "14     2020-07-02T19:00:11Z  UCBc0Mghy-6jhMXs3T7DluMg       11051         300   \n",
      "15     2020-07-02T09:30:06Z  UCaCaZ-vKtnMG2_FKmEePChQ        8625         432   \n",
      "...                     ...                       ...         ...         ...   \n",
      "11204  2022-02-20T11:32:10Z  UCQOnq7u47LRBgWXr1Xc-XWQ        4196         143   \n",
      "11207  2022-02-05T15:59:43Z  UCpPbcfwq0wOpWrC0gBs73tQ       16111         858   \n",
      "11208  2022-02-06T13:00:11Z  UC0wANY47B2CDa1_9ByPzz_A        5288         147   \n",
      "11210  2022-02-18T16:00:11Z  UCpPbcfwq0wOpWrC0gBs73tQ       16094         749   \n",
      "11212  2022-02-22T14:30:05Z  UCDDj2GWklzZ09X7R9OtsX3Q        5237         295   \n",
      "\n",
      "       comment_count  job  id  valid     month  \n",
      "2                120    0   0  valid      July  \n",
      "5                826    0   0  valid      July  \n",
      "11              4747    0   0  valid      July  \n",
      "14                63    0   0  valid      July  \n",
      "15               127    0   0  valid      July  \n",
      "...              ...  ...  ..    ...       ...  \n",
      "11204             52    0   0  valid  February  \n",
      "11207             51    0   0  valid  February  \n",
      "11208             51    0   0  valid  February  \n",
      "11210             51    0   0  valid  February  \n",
      "11212             51    0   0  valid  February  \n",
      "\n",
      "[3783 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(og_merged_df)\n",
    "columns = ['view_count', 'like_count']\n",
    "og_merged_df[columns] = og_merged_df[columns].fillna(0).astype(int)\n",
    "print(og_merged_df)\n",
    "\n",
    "\n",
    "unique_channels_lst = merged_df['channel_id'].tolist()\n",
    "min_views_lst = []\n",
    "max_views_lst = []\n",
    "avg_views_lst = []\n",
    "\n",
    "min_likes_lst = []\n",
    "max_likes_lst = []\n",
    "avg_likes_lst = []\n",
    "\n",
    "min_comments_lst = []\n",
    "max_comments_lst = []\n",
    "avg_comments_lst = []\n",
    "\n",
    "for channel in unique_channels_lst:\n",
    "    selected_channel_df = og_merged_df.loc[og_merged_df['channel_id'] == channel]\n",
    "    min_views = selected_channel_df['view_count'].min()\n",
    "    max_views = selected_channel_df['view_count'].max()\n",
    "    avg_views = selected_channel_df['view_count'].mean()\n",
    "    \n",
    "    min_likes = selected_channel_df['like_count'].min()\n",
    "    max_likes = selected_channel_df['like_count'].max()\n",
    "    avg_likes = selected_channel_df['like_count'].mean()\n",
    "    \n",
    "    min_comments = selected_channel_df['comment_count'].min()\n",
    "    max_comments = selected_channel_df['comment_count'].max()\n",
    "    avg_comments = selected_channel_df['comment_count'].mean()\n",
    "    \n",
    "    min_views_lst.append(min_views)\n",
    "    max_views_lst.append(max_views)\n",
    "    avg_views_lst.append(avg_views)\n",
    "    \n",
    "    min_likes_lst.append(min_likes)\n",
    "    max_likes_lst.append(max_likes)\n",
    "    avg_likes_lst.append(avg_likes)\n",
    "    \n",
    "    min_comments_lst.append(min_comments)\n",
    "    max_comments_lst.append(max_comments)\n",
    "    avg_comments_lst.append(avg_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6d2378e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['min_views'] = min_views_lst\n",
    "merged_df['max_views'] = max_views_lst\n",
    "merged_df['avg_views'] = avg_views_lst\n",
    "\n",
    "merged_df['min_likes'] = min_likes_lst\n",
    "merged_df['max_likes'] = max_likes_lst\n",
    "merged_df['avg_likes'] = avg_likes_lst\n",
    "\n",
    "merged_df['min_comments'] = min_comments_lst\n",
    "merged_df['max_comments'] = max_comments_lst\n",
    "merged_df['avg_comments'] = avg_comments_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "14c1d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"grouped_df_with_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ccd3f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuzzy decision making: Measuring User Influence Based on Multiple Metrics on YouTube\n",
    "# TOPSIS; Fuzzy decision making origins: Fuzzy Multiple Attribute Decision Making Methods\n",
    "# TOPSIS Python: https://pypi.org/project/topsis-jamesfallon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "fb2b17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 741584.           1167135.            961788.6666666666 ...\n",
      "     2177.              3282.              2739.          ]\n",
      " [  38746.            387902.            178695.3333333333 ...\n",
      "      115.               966.               562.7777777778]\n",
      " [  22394.            137175.             75113.6          ...\n",
      "      120.              1360.               535.8         ]\n",
      " ...\n",
      " [  44803.            389656.            132763.1428571429 ...\n",
      "      120.              1126.               394.6428571429]\n",
      " [  18506.            184217.             88668.75         ...\n",
      "       56.               555.               412.          ]\n",
      " [  10219.             16740.             13479.5          ...\n",
      "       62.                67.                64.5         ]]\n",
      "     min_views  max_views  avg_views  min_likes  max_likes  avg_likes  \\\n",
      "0     0.078219   0.022048   0.063548   0.064990   0.026532   0.054659   \n",
      "1     0.004087   0.007328   0.011807   0.002159   0.004981   0.006717   \n",
      "2     0.002362   0.002591   0.004963   0.000431   0.001734   0.001815   \n",
      "3     0.000596   0.000130   0.000404   0.000331   0.000154   0.000319   \n",
      "4     0.000831   0.000793   0.001647   0.000203   0.000593   0.000754   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "203   0.000253   0.001579   0.000706   0.000536   0.001714   0.001227   \n",
      "204   0.000338   0.000531   0.001035   0.000271   0.000528   0.000706   \n",
      "205   0.004726   0.007361   0.008772   0.002440   0.006835   0.004843   \n",
      "206   0.001952   0.003480   0.005859   0.002420   0.004350   0.006177   \n",
      "207   0.001078   0.000316   0.000891   0.001087   0.000466   0.000952   \n",
      "\n",
      "     min_comments  max_comments  avg_comments  \n",
      "0        0.049078      0.016662      0.040502  \n",
      "1        0.002593      0.004904      0.008322  \n",
      "2        0.002705      0.006904      0.007923  \n",
      "3        0.001398      0.000498      0.001109  \n",
      "4        0.001172      0.000472      0.001072  \n",
      "..            ...           ...           ...  \n",
      "203      0.001172      0.004747      0.002242  \n",
      "204      0.001398      0.000650      0.001405  \n",
      "205      0.002705      0.005716      0.005836  \n",
      "206      0.001262      0.002818      0.006092  \n",
      "207      0.001398      0.000340      0.000954  \n",
      "\n",
      "[208 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "numpy_array = merged_df[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "\n",
    "print(numpy_array)\n",
    "# Step 1: Calculate the normalized decision matrix\n",
    "\n",
    "# Square all values in column\n",
    "numpy_square = np.square(numpy_array)\n",
    "\n",
    "# Get the sum of squares and then take the squareroute of it\n",
    "sqrt_sum_0 = math.sqrt(np.sum(numpy_square[:, 0]))\n",
    "sqrt_sum_1 = math.sqrt(np.sum(numpy_square[:, 1]))\n",
    "sqrt_sum_2 = math.sqrt(np.sum(numpy_square[:, 2]))\n",
    "sqrt_sum_3 = math.sqrt(np.sum(numpy_square[:, 3]))\n",
    "sqrt_sum_4 = math.sqrt(np.sum(numpy_square[:, 4]))\n",
    "sqrt_sum_5 = math.sqrt(np.sum(numpy_square[:, 5]))\n",
    "sqrt_sum_6 = math.sqrt(np.sum(numpy_square[:, 6]))\n",
    "sqrt_sum_7 = math.sqrt(np.sum(numpy_square[:, 7]))\n",
    "sqrt_sum_8 = math.sqrt(np.sum(numpy_square[:, 8]))\n",
    "\n",
    "column_0 = numpy_array[:, 0]\n",
    "column_1 = numpy_array[:, 1]\n",
    "column_2 = numpy_array[:, 2]\n",
    "column_3 = numpy_array[:, 3]\n",
    "column_4 = numpy_array[:, 4]\n",
    "column_5 = numpy_array[:, 5]\n",
    "column_6 = numpy_array[:, 6]\n",
    "column_7 = numpy_array[:, 7]\n",
    "column_8 = numpy_array[:, 8]\n",
    "\n",
    "# Normalize by taking each row value and dividing it by the before calculated squareroot sum\n",
    "f = lambda x: x/sqrt_sum\n",
    "\n",
    "sqrt_sum = sqrt_sum_0\n",
    "nor_column_0 = f(column_0)\n",
    "sqrt_sum = sqrt_sum_1\n",
    "nor_column_1 = f(column_1)\n",
    "sqrt_sum = sqrt_sum_2\n",
    "nor_column_2 = f(column_2)\n",
    "sqrt_sum = sqrt_sum_3\n",
    "nor_column_3 = f(column_3)\n",
    "sqrt_sum = sqrt_sum_4\n",
    "nor_column_4 = f(column_4)\n",
    "sqrt_sum = sqrt_sum_5\n",
    "nor_column_5 = f(column_5)\n",
    "sqrt_sum = sqrt_sum_6\n",
    "nor_column_6 = f(column_6)\n",
    "sqrt_sum = sqrt_sum_7\n",
    "nor_column_7 = f(column_7)\n",
    "sqrt_sum = sqrt_sum_8\n",
    "nor_column_8 = f(column_8)\n",
    "\n",
    "# Step 2: Calculate the weighted decision matrix\n",
    "# Weight of 1/9 because of 9 attributes and Xiao etal. 2015: \"We suppose each metric has the same weight\"\n",
    "weight = 1/9\n",
    "\n",
    "weight_function = lambda x: x*weight\n",
    "weight_column_0 = weight_function(nor_column_0)\n",
    "weight_column_1 = weight_function(nor_column_1)\n",
    "weight_column_2 = weight_function(nor_column_2)\n",
    "weight_column_3 = weight_function(nor_column_3)\n",
    "weight_column_4 = weight_function(nor_column_4)\n",
    "weight_column_5 = weight_function(nor_column_5)\n",
    "weight_column_6 = weight_function(nor_column_6)\n",
    "weight_column_7 = weight_function(nor_column_7)\n",
    "weight_column_8 = weight_function(nor_column_8)\n",
    "\n",
    "# Step 3: Determine the ideal and negative-ideal solutions:\n",
    "max_column_0 = np.max(weight_column_0)\n",
    "max_column_1 = np.max(weight_column_1)\n",
    "max_column_2 = np.max(weight_column_2)\n",
    "max_column_3 = np.max(weight_column_3)\n",
    "max_column_4 = np.max(weight_column_4)\n",
    "max_column_5 = np.max(weight_column_5)\n",
    "max_column_6 = np.max(weight_column_6)\n",
    "max_column_7 = np.max(weight_column_7)\n",
    "max_column_8 = np.max(weight_column_8)\n",
    "\n",
    "min_column_0 = np.min(weight_column_0)\n",
    "min_column_1 = np.min(weight_column_1)\n",
    "min_column_2 = np.min(weight_column_2)\n",
    "min_column_3 = np.min(weight_column_3)\n",
    "min_column_4 = np.min(weight_column_4)\n",
    "min_column_5 = np.min(weight_column_5)\n",
    "min_column_6 = np.min(weight_column_6)\n",
    "min_column_7 = np.min(weight_column_7)\n",
    "min_column_8 = np.min(weight_column_8)\n",
    "\n",
    "ideal_max = (max_column_0, max_column_1, max_column_2, max_column_3, max_column_4, max_column_5, max_column_6, max_column_7, max_column_8)\n",
    "ideal_min = (min_column_0, min_column_1, min_column_2, min_column_3, min_column_4, min_column_5, min_column_6, min_column_7, min_column_8)\n",
    "\n",
    "# Step 4: Calculate the separation measures \n",
    "\n",
    "weighted_df = pd.DataFrame({'min_views': weight_column_0, 'max_views': weight_column_1, 'avg_views': weight_column_2, 'min_likes': weight_column_3, 'max_likes': weight_column_4, 'avg_likes': weight_column_5, 'min_comments': weight_column_6, 'max_comments': weight_column_7, 'avg_comments': weight_column_8})\n",
    "print(weighted_df)\n",
    "\n",
    "#weighted_array = weighted_df[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "\n",
    "#print(weighted_array)\n",
    "\n",
    "#euc_pos = lambda x: distance.euclidean(ideal_max, x)\n",
    "\n",
    "#ideal_pos_array = euc_pos(weighted_array)\n",
    "\n",
    "#print(ideal_pos_array)\n",
    "def euclidian_max(a, b, c, d, e, f, g, h, i):\n",
    "    row_vector = (a, b, c, d, e, f, g, h, i)\n",
    "    return distance.euclidean(row_vector, ideal_max)\n",
    "\n",
    "def euclidian_min(a, b, c, d, e, f, g, h, i):\n",
    "    row_vector = (a, b, c, d, e, f, g, h, i)\n",
    "    return distance.euclidean(row_vector, ideal_min)\n",
    "\n",
    "\n",
    "weighted_df['euc_pos'] = weighted_df.apply(lambda row : euclidian_max(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "weighted_df['euc_neg'] = weighted_df.apply(lambda row : euclidian_min(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "\n",
    "# Step 5: Calculate the relative closeness to the ideal solution:\n",
    "def relative_closeness(pos, neg):\n",
    "    relative = neg/(pos+neg)\n",
    "    \n",
    "    return relative\n",
    "\n",
    "weighted_df['euc_rel'] = weighted_df.apply(lambda row : relative_closeness(row['euc_pos'],\n",
    "                     row['euc_neg']), axis = 1)\n",
    "\n",
    "\n",
    "# Step 6: Rank the preference order:\n",
    "channel_id = merged_df['channel_id'].tolist()\n",
    "channel_title = merged_df['channel_title'].tolist()\n",
    "weighted_df['channel_id'] = channel_id\n",
    "weighted_df['channel_title'] = channel_title\n",
    "weighted_df = weighted_df.sort_values(by='euc_rel', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "3a0362ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_views</th>\n",
       "      <th>max_views</th>\n",
       "      <th>avg_views</th>\n",
       "      <th>min_likes</th>\n",
       "      <th>max_likes</th>\n",
       "      <th>avg_likes</th>\n",
       "      <th>min_comments</th>\n",
       "      <th>max_comments</th>\n",
       "      <th>avg_comments</th>\n",
       "      <th>euc_pos</th>\n",
       "      <th>euc_neg</th>\n",
       "      <th>euc_rel</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.078219</td>\n",
       "      <td>0.022048</td>\n",
       "      <td>0.063548</td>\n",
       "      <td>0.064990</td>\n",
       "      <td>0.026532</td>\n",
       "      <td>0.054659</td>\n",
       "      <td>0.049078</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>0.040502</td>\n",
       "      <td>0.111772</td>\n",
       "      <td>0.150551</td>\n",
       "      <td>0.573915</td>\n",
       "      <td>UC1-VOKyTJrgLiBeiJqzeIUQ</td>\n",
       "      <td>JP Performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.091828</td>\n",
       "      <td>0.036844</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.080605</td>\n",
       "      <td>0.021075</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.084934</td>\n",
       "      <td>0.030468</td>\n",
       "      <td>0.124790</td>\n",
       "      <td>0.157337</td>\n",
       "      <td>0.557682</td>\n",
       "      <td>UCcweJsCV2TUP_kzMX25U-oQ</td>\n",
       "      <td>BR24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.040320</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>0.033237</td>\n",
       "      <td>0.068292</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>0.056771</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.130301</td>\n",
       "      <td>0.124792</td>\n",
       "      <td>0.489203</td>\n",
       "      <td>UCzH549YlZhdhIqhtvz7XHmQ</td>\n",
       "      <td>AlexiBexi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>0.024895</td>\n",
       "      <td>0.006854</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>0.048988</td>\n",
       "      <td>0.012687</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0.167848</td>\n",
       "      <td>0.073792</td>\n",
       "      <td>0.305381</td>\n",
       "      <td>UCn7wWR5KnpX_N6ZaBNuyVYw</td>\n",
       "      <td>WDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.016285</td>\n",
       "      <td>0.010619</td>\n",
       "      <td>0.021255</td>\n",
       "      <td>0.019535</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.024889</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>0.020733</td>\n",
       "      <td>0.037183</td>\n",
       "      <td>0.157935</td>\n",
       "      <td>0.065015</td>\n",
       "      <td>0.291612</td>\n",
       "      <td>UCcnmNzv0yEEYPM6Oa86unhQ</td>\n",
       "      <td>Die Autodoktoren - offizieller Kanal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.023594</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.030903</td>\n",
       "      <td>0.016309</td>\n",
       "      <td>0.030537</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.025279</td>\n",
       "      <td>0.159318</td>\n",
       "      <td>0.065459</td>\n",
       "      <td>0.291217</td>\n",
       "      <td>UCkeQaTKHKmRnWmXnl0uIuSA</td>\n",
       "      <td>Maeximiliano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.014349</td>\n",
       "      <td>0.010890</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.023182</td>\n",
       "      <td>0.012049</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.033435</td>\n",
       "      <td>0.017116</td>\n",
       "      <td>0.172241</td>\n",
       "      <td>0.048914</td>\n",
       "      <td>0.221176</td>\n",
       "      <td>UCiZnK4X73okItJqaCs2YDOQ</td>\n",
       "      <td>Car Maniac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>0.019933</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.007219</td>\n",
       "      <td>0.012087</td>\n",
       "      <td>0.170149</td>\n",
       "      <td>0.046191</td>\n",
       "      <td>0.213513</td>\n",
       "      <td>UCM1jNA8DM90LE4ywxBe8RmA</td>\n",
       "      <td>Spiel und Zeug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.015998</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.012746</td>\n",
       "      <td>0.021920</td>\n",
       "      <td>0.012873</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>0.014107</td>\n",
       "      <td>0.174251</td>\n",
       "      <td>0.043116</td>\n",
       "      <td>0.198356</td>\n",
       "      <td>UC173FCaSv6VvqDxuoinDr3g</td>\n",
       "      <td>Felixba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.011016</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>0.018560</td>\n",
       "      <td>0.017637</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>0.009123</td>\n",
       "      <td>0.014915</td>\n",
       "      <td>0.173156</td>\n",
       "      <td>0.041228</td>\n",
       "      <td>0.192309</td>\n",
       "      <td>UCE2hJ9CYR57BYhk3TjGVG6w</td>\n",
       "      <td>Breaking Lab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.004417</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.029149</td>\n",
       "      <td>0.011697</td>\n",
       "      <td>0.025010</td>\n",
       "      <td>0.182910</td>\n",
       "      <td>0.043272</td>\n",
       "      <td>0.191314</td>\n",
       "      <td>UCBoIJhUfNcn8G_odWECKVIA</td>\n",
       "      <td>Dr. Dirk Spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.012410</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.178823</td>\n",
       "      <td>0.038766</td>\n",
       "      <td>0.178161</td>\n",
       "      <td>UChr1sKJ27Fr6WZxAlusbo_w</td>\n",
       "      <td>Felix von der Laden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.019174</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.017513</td>\n",
       "      <td>0.012227</td>\n",
       "      <td>0.020705</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.180097</td>\n",
       "      <td>0.038985</td>\n",
       "      <td>0.177946</td>\n",
       "      <td>UCPFhnt7T8iaBLp1wIfFRzFw</td>\n",
       "      <td>Johnny Hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.014878</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.182026</td>\n",
       "      <td>0.035393</td>\n",
       "      <td>0.162789</td>\n",
       "      <td>UCLINPbYQ9sy6qc-TqtBeVnw</td>\n",
       "      <td>auto motor und sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.025640</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>0.016643</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>0.186370</td>\n",
       "      <td>0.034287</td>\n",
       "      <td>0.155385</td>\n",
       "      <td>UCeeHT-3WejdX6uGBgCbE7SA</td>\n",
       "      <td>TopSpeedGermany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     min_views  max_views  avg_views  min_likes  max_likes  avg_likes  \\\n",
       "0     0.078219   0.022048   0.063548   0.064990   0.026532   0.054659   \n",
       "107   0.000368   0.091828   0.036844   0.000150   0.080605   0.021075   \n",
       "191   0.040320   0.011784   0.033237   0.068292   0.026638   0.056771   \n",
       "164   0.033074   0.008312   0.024895   0.006854   0.002930   0.005990   \n",
       "93    0.016285   0.010619   0.021255   0.019535   0.016394   0.024889   \n",
       "132   0.023594   0.005109   0.016326   0.030903   0.016309   0.030537   \n",
       "162   0.002946   0.014349   0.010890   0.003953   0.023182   0.012049   \n",
       "52    0.009407   0.018526   0.019933   0.009908   0.023834   0.020882   \n",
       "36    0.011763   0.007737   0.015998   0.020414   0.012746   0.021920   \n",
       "82    0.010488   0.011016   0.013419   0.012786   0.018560   0.017637   \n",
       "57    0.004417   0.003378   0.006658   0.010687   0.007252   0.011671   \n",
       "160   0.010769   0.006073   0.012410   0.015700   0.017765   0.022828   \n",
       "102   0.019174   0.004814   0.014424   0.017513   0.012227   0.020705   \n",
       "70    0.001615   0.022667   0.010392   0.000819   0.014878   0.005254   \n",
       "163   0.025640   0.004925   0.016643   0.010336   0.004031   0.008591   \n",
       "\n",
       "     min_comments  max_comments  avg_comments   euc_pos   euc_neg   euc_rel  \\\n",
       "0        0.049078      0.016662      0.040502  0.111772  0.150551  0.573915   \n",
       "107      0.001172      0.084934      0.030468  0.124790  0.157337  0.557682   \n",
       "191      0.053903      0.012276      0.035556  0.130301  0.124792  0.489203   \n",
       "164      0.048988      0.012687      0.034543  0.167848  0.073792  0.305381   \n",
       "93       0.021124      0.020733      0.037183  0.157935  0.065015  0.291612   \n",
       "132      0.024934      0.011742      0.025279  0.159318  0.065459  0.291217   \n",
       "162      0.003021      0.033435      0.017116  0.172241  0.048914  0.221176   \n",
       "52       0.007282      0.007219      0.012087  0.170149  0.046191  0.213513   \n",
       "36       0.012873      0.006686      0.014107  0.174251  0.043116  0.198356   \n",
       "82       0.015330      0.009123      0.014915  0.173156  0.041228  0.192309   \n",
       "57       0.029149      0.011697      0.025010  0.182910  0.043272  0.191314   \n",
       "160      0.004058      0.006574      0.009400  0.178823  0.038766  0.178161   \n",
       "102      0.006042      0.002102      0.005042  0.180097  0.038985  0.177946   \n",
       "70       0.002435      0.017185      0.010650  0.182026  0.035393  0.162789   \n",
       "163      0.005095      0.001548      0.003926  0.186370  0.034287  0.155385   \n",
       "\n",
       "                   channel_id                         channel_title  \n",
       "0    UC1-VOKyTJrgLiBeiJqzeIUQ                        JP Performance  \n",
       "107  UCcweJsCV2TUP_kzMX25U-oQ                                  BR24  \n",
       "191  UCzH549YlZhdhIqhtvz7XHmQ                             AlexiBexi  \n",
       "164  UCn7wWR5KnpX_N6ZaBNuyVYw                                   WDR  \n",
       "93   UCcnmNzv0yEEYPM6Oa86unhQ  Die Autodoktoren - offizieller Kanal  \n",
       "132  UCkeQaTKHKmRnWmXnl0uIuSA                          Maeximiliano  \n",
       "162  UCiZnK4X73okItJqaCs2YDOQ                            Car Maniac  \n",
       "52   UCM1jNA8DM90LE4ywxBe8RmA                        Spiel und Zeug  \n",
       "36   UC173FCaSv6VvqDxuoinDr3g                               Felixba  \n",
       "82   UCE2hJ9CYR57BYhk3TjGVG6w                          Breaking Lab  \n",
       "57   UCBoIJhUfNcn8G_odWECKVIA                      Dr. Dirk Spaniel  \n",
       "160  UChr1sKJ27Fr6WZxAlusbo_w                   Felix von der Laden  \n",
       "102  UCPFhnt7T8iaBLp1wIfFRzFw                           Johnny Hand  \n",
       "70   UCLINPbYQ9sy6qc-TqtBeVnw                  auto motor und sport  \n",
       "163  UCeeHT-3WejdX6uGBgCbE7SA                       TopSpeedGermany  "
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_df.to_csv('influence_euc.csv', index=False)\n",
    "\n",
    "weighted_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "00f0f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"C:/Users/fabia/OneDrive - Otto-Friedrich-Universität Bamberg/Master/Masterarbeit/Data/Video/\"\n",
    "\n",
    "videos_w_topics = pd.read_csv(work_dir + \"video_topics.csv\")\n",
    "\n",
    "videos_w_topics.drop(['valid', 'job', 'id'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9f226f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_w_topics = pd.merge(videos_w_topics,weighted_df,on='channel_id',how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "334d55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3491, 22)\n",
      "(3491, 22)\n"
     ]
    }
   ],
   "source": [
    "print(influence_w_topics.shape)\n",
    "influence_w_topics =influence_w_topics[influence_w_topics['euc_rel'].notna()]\n",
    "print(influence_w_topics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f12c7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 14)\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "print(weighted_df.shape)\n",
    "\n",
    "group = influence_w_topics.groupby('channel_title')\n",
    "print(group.ngroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a223fc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      topic     video_id                                              title  \\\n",
      "0         2  vYzSzy6IwRI  E-AUTO mit 100% WASSERKRAFT laden | Prototyp n...   \n",
      "1         2  Ad8knG-9l2A  Breaking Elektro-News: Xiaomi plant Elektroaut...   \n",
      "2         1  SVyPVVXFpkU  Model 3 Facelift (SR & LR): Analyse der Tests ...   \n",
      "3         3  z67yc8r2WNw  Probleme und Mängel an meinem Audi E-Tron, Upd...   \n",
      "4         0  71SbVFBhaZk  STAT E-STICS #12 | KIA EV6 und alle Elektroaut...   \n",
      "...     ...          ...                                                ...   \n",
      "3740      0  JwzOlPBheO0                  Lucid Air unveiling in 10 minutes   \n",
      "3741      1  ZSouH4TSg30  Tesla Supercharger: VW, BMW + Co. Gratis-CCS-L...   \n",
      "3742      0  GDdHPUOu2yg  Peugeot E 208 - Kleines E-Auto mit guter Preis...   \n",
      "3743      0  czuwJbYMXf8  Opel Mokka (2021): E-SUV zeigt neues Markenges...   \n",
      "3744      0  ewFGmnjOkW0  VW ID.4 ab 30.000€ I Porsche E-Autos beliebter...   \n",
      "\n",
      "              published_at                channel_id  view_count  like_count  \\\n",
      "0     2021-04-01T08:00:27Z  UCaCaZ-vKtnMG2_FKmEePChQ      4217.0       493.0   \n",
      "1     2021-04-01T05:00:02Z  UCsNdsFTTBj5jcRJSWeJ8-Xg      7432.0       613.0   \n",
      "2     2021-04-01T16:15:55Z  UCasePAQ_q7tbUGybICCj4NQ      5530.0       342.0   \n",
      "3     2021-04-02T17:11:32Z  UCddkvrnYqGRfODPMUQqisiw      6389.0       291.0   \n",
      "4     2021-04-02T15:30:01Z  UCDDj2GWklzZ09X7R9OtsX3Q     38400.0       508.0   \n",
      "...                    ...                       ...         ...         ...   \n",
      "3740  2020-09-10T03:58:09Z  UC-6OW5aJYBFM33zXQlBKPNA    961035.0     17457.0   \n",
      "3741  2020-09-11T17:21:01Z  UCSwRhu8CeHVXZlG1ExdjuTQ     10055.0       384.0   \n",
      "3742  2020-09-16T08:00:02Z  UCJW1S6BOOHD1dyzA2V8rxnw      7382.0       201.0   \n",
      "3743  2020-09-25T07:00:11Z  UCLINPbYQ9sy6qc-TqtBeVnw     26906.0       544.0   \n",
      "3744  2020-09-25T17:14:37Z  UCSwRhu8CeHVXZlG1ExdjuTQ     11222.0       352.0   \n",
      "\n",
      "      comment_count                                    title_processed  \n",
      "0               193  e-auto mit 100% wasserkraft laden | prototyp n...  \n",
      "1               129  breaking elektro-news: xiaomi plant elektroaut...  \n",
      "2                88  model 3 facelift (sr & lr): analyse der tests ...  \n",
      "3                89  probleme und mängel an meinem audi e-tron upda...  \n",
      "4                99  stat e-stics #12 | kia ev6 und alle elektroaut...  \n",
      "...             ...                                                ...  \n",
      "3740           2319                  lucid air unveiling in 10 minutes  \n",
      "3741             87  tesla supercharger: vw bmw + co gratis-ccs-lad...  \n",
      "3742             67  peugeot e 208 - kleines e-auto mit guter preis...  \n",
      "3743            128  opel mokka (2021): e-suv zeigt neues markenges...  \n",
      "3744            136  vw id4 ab 30000€ i porsche e-autos beliebter a...  \n",
      "\n",
      "[3745 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(videos_w_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "ec532aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   channel_id        channel_title  topic  counts  min_views  \\\n",
      "0    UC-k-5F2j4CUzuwkx1QLRlpA  gewaltig nachhaltig      1       9   0.000142   \n",
      "1    UC-k-5F2j4CUzuwkx1QLRlpA  gewaltig nachhaltig      2      16   0.000142   \n",
      "2    UC-k-5F2j4CUzuwkx1QLRlpA  gewaltig nachhaltig      3       8   0.000142   \n",
      "3    UC-r2l4o0_R3eRwOHthmGAcA           Tesla Kägi      0       1   0.000165   \n",
      "4    UC-r2l4o0_R3eRwOHthmGAcA           Tesla Kägi      2       1   0.000165   \n",
      "..                        ...                  ...    ...     ...        ...   \n",
      "486  UCzH549YlZhdhIqhtvz7XHmQ            AlexiBexi      0       1   0.040320   \n",
      "487  UCzH549YlZhdhIqhtvz7XHmQ            AlexiBexi      3       1   0.040320   \n",
      "488  UCzZ0tUcFR7APjPKdxmgkbJA             JOCR Raw      1       1   0.002633   \n",
      "489  UCzZ0tUcFR7APjPKdxmgkbJA             JOCR Raw      3       1   0.002633   \n",
      "490  UCzyBlIm4NE2ch85fY3Y4cOA         asphalt. art      0      11   0.000986   \n",
      "\n",
      "     max_views  avg_views  min_likes  max_likes  avg_likes  min_comments  \\\n",
      "0     0.001354   0.000920   0.000321   0.001100   0.001002      0.001217   \n",
      "1     0.001354   0.000920   0.000321   0.001100   0.001002      0.001217   \n",
      "2     0.001354   0.000920   0.000321   0.001100   0.001002      0.001217   \n",
      "3     0.000117   0.000257   0.000085   0.000122   0.000172      0.001172   \n",
      "4     0.000117   0.000257   0.000085   0.000122   0.000172      0.001172   \n",
      "..         ...        ...        ...        ...        ...           ...   \n",
      "486   0.011784   0.033237   0.068292   0.026638   0.056771      0.053903   \n",
      "487   0.011784   0.033237   0.068292   0.026638   0.056771      0.053903   \n",
      "488   0.000631   0.001928   0.002618   0.001205   0.002386      0.002660   \n",
      "489   0.000631   0.001928   0.002618   0.001205   0.002386      0.002660   \n",
      "490   0.013345   0.007028   0.000529   0.005435   0.002418      0.001353   \n",
      "\n",
      "     max_comments  avg_comments   euc_pos   euc_neg    euc_rel  \n",
      "0        0.001960      0.001922  0.208337  0.002924   0.014236  \n",
      "1        0.001960      0.001922  0.208337  0.002924   0.014236  \n",
      "2        0.001960      0.001922  0.208337  0.002924   0.014236  \n",
      "3        0.000315      0.000843  0.210609  0.000286   0.001359  \n",
      "4        0.000315      0.000843  0.210609  0.000286   0.001359  \n",
      "..            ...           ...       ...       ...        ...  \n",
      "486      0.012276      0.035556  0.130301  0.124792  22.653728  \n",
      "487      0.012276      0.035556  0.130301  0.124792  22.653728  \n",
      "488      0.000680      0.001863  0.206446  0.005235   0.026017  \n",
      "489      0.000680      0.001863  0.206446  0.005235   0.026017  \n",
      "490      0.004351      0.003312  0.197873  0.016855   0.093115  \n",
      "\n",
      "[491 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "grouped = influence_w_topics.groupby(['channel_id', 'channel_title', 'topic']).size().reset_index(name='counts')\n",
    "\n",
    "grouped_merge = grouped.merge(weighted_df)\n",
    "print(grouped_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "fbac2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_merge.to_csv(\"grouped_merged_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "5c072566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(134, 4)\n",
      "(112, 4)\n",
      "(133, 4)\n",
      "                   channel_id          channel_title  topic  counts\n",
      "3    UC-r2l4o0_R3eRwOHthmGAcA             Tesla Kägi      0       1\n",
      "11   UC0cCZpU5Ioj1FOuIzj7cSsQ  Speicher elektrisiert      0       4\n",
      "15   UC1-VOKyTJrgLiBeiJqzeIUQ         JP Performance      0       2\n",
      "17   UC173FCaSv6VvqDxuoinDr3g                Felixba      0       1\n",
      "23   UC1w6pNGiiLdZgyNpXUnA4Zw            DER SPIEGEL      0       2\n",
      "..                        ...                    ...    ...     ...\n",
      "471  UCwjxya3zbM2yadFag9WlbYg           GRIP Elektro      0      14\n",
      "474  UCxmohYAjhc2qD5pYIOwQ3bA         electric drive      0      20\n",
      "481  UCy9Y4en2y1eR1zwcmFkRMzg   the car crash review      0      99\n",
      "486  UCzH549YlZhdhIqhtvz7XHmQ              AlexiBexi      0       1\n",
      "490  UCzyBlIm4NE2ch85fY3Y4cOA           asphalt. art      0      11\n",
      "\n",
      "[112 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2982704402.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2982704402.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2982704402.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2982704402.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "topic_1 = grouped_merge.loc[(grouped_merge[\"topic\"] == 0)]\n",
    "topic_2 = grouped_merge.loc[(grouped_merge[\"topic\"] == 1)]\n",
    "topic_3 = grouped_merge.loc[(grouped_merge[\"topic\"] == 2)]\n",
    "topic_4 = grouped_merge.loc[(grouped_merge[\"topic\"] == 3)]\n",
    "\n",
    "topic_1.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n",
    "topic_2.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n",
    "topic_3.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n",
    "topic_4.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'euc_pos', 'euc_neg', 'euc_rel'], axis=1, inplace=True)\n",
    "\n",
    "print(topic_1.shape)\n",
    "print(topic_2.shape)\n",
    "print(topic_3.shape)\n",
    "print(topic_4.shape)\n",
    "\n",
    "print(topic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "0429bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['min_views'] = min_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['max_views'] = max_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['avg_views'] = avg_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['min_likes'] = min_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['max_likes'] = max_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['avg_likes'] = avg_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['min_comments'] = min_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['max_comments'] = max_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_1['avg_comments'] = avg_comments_lst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['min_views'] = min_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['max_views'] = max_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['avg_views'] = avg_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['min_likes'] = min_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['max_likes'] = max_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['avg_likes'] = avg_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['min_comments'] = min_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['max_comments'] = max_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_2['avg_comments'] = avg_comments_lst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['min_views'] = min_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['max_views'] = max_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['avg_views'] = avg_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:151: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['min_likes'] = min_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['max_likes'] = max_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['avg_likes'] = avg_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['min_comments'] = min_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['max_comments'] = max_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_3['avg_comments'] = avg_comments_lst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:198: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['min_views'] = min_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['max_views'] = max_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:200: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['avg_views'] = avg_views_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['min_likes'] = min_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['max_likes'] = max_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['avg_likes'] = avg_likes_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:206: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['min_comments'] = min_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:207: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['max_comments'] = max_comments_lst\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\2826199879.py:208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_4['avg_comments'] = avg_comments_lst\n"
     ]
    }
   ],
   "source": [
    "topic_1_channel_id = topic_1['channel_id'].tolist()\n",
    "topic_2_channel_id = topic_2['channel_id'].tolist()\n",
    "topic_3_channel_id = topic_3['channel_id'].tolist()\n",
    "topic_4_channel_id = topic_4['channel_id'].tolist()\n",
    "\n",
    "min_views_lst = []\n",
    "max_views_lst = []\n",
    "avg_views_lst = []\n",
    "\n",
    "min_likes_lst = []\n",
    "max_likes_lst = []\n",
    "avg_likes_lst = []\n",
    "\n",
    "min_comments_lst = []\n",
    "max_comments_lst = []\n",
    "avg_comments_lst = []\n",
    "# Topic 1\n",
    "print(len(topic_1_channel_id))\n",
    "for channel in topic_1_channel_id:\n",
    "    selected_channel_df = videos_w_topics.loc[videos_w_topics['channel_id'] == channel]\n",
    "    min_views = selected_channel_df['view_count'].min()\n",
    "    max_views = selected_channel_df['view_count'].max()\n",
    "    avg_views = selected_channel_df['view_count'].mean()\n",
    "    \n",
    "    min_likes = selected_channel_df['like_count'].min()\n",
    "    max_likes = selected_channel_df['like_count'].max()\n",
    "    avg_likes = selected_channel_df['like_count'].mean()\n",
    "    \n",
    "    min_comments = selected_channel_df['comment_count'].min()\n",
    "    max_comments = selected_channel_df['comment_count'].max()\n",
    "    avg_comments = selected_channel_df['comment_count'].mean()\n",
    "    \n",
    "    min_views_lst.append(min_views)\n",
    "    max_views_lst.append(max_views)\n",
    "    avg_views_lst.append(avg_views)\n",
    "    \n",
    "    min_likes_lst.append(min_likes)\n",
    "    max_likes_lst.append(max_likes)\n",
    "    avg_likes_lst.append(avg_likes)\n",
    "    \n",
    "    min_comments_lst.append(min_comments)\n",
    "    max_comments_lst.append(max_comments)\n",
    "    avg_comments_lst.append(avg_comments)\n",
    "    \n",
    "topic_1['min_views'] = min_views_lst\n",
    "topic_1['max_views'] = max_views_lst\n",
    "topic_1['avg_views'] = avg_views_lst\n",
    "\n",
    "topic_1['min_likes'] = min_likes_lst\n",
    "topic_1['max_likes'] = max_likes_lst\n",
    "topic_1['avg_likes'] = avg_likes_lst\n",
    "\n",
    "topic_1['min_comments'] = min_comments_lst\n",
    "topic_1['max_comments'] = max_comments_lst\n",
    "topic_1['avg_comments'] = avg_comments_lst\n",
    "\n",
    "min_views_lst = []\n",
    "max_views_lst = []\n",
    "avg_views_lst = []\n",
    "\n",
    "min_likes_lst = []\n",
    "max_likes_lst = []\n",
    "avg_likes_lst = []\n",
    "\n",
    "min_comments_lst = []\n",
    "max_comments_lst = []\n",
    "avg_comments_lst = []\n",
    "# Topic 2\n",
    "print(len(topic_2_channel_id))\n",
    "for channel in topic_2_channel_id:\n",
    "    selected_channel_df = videos_w_topics.loc[videos_w_topics['channel_id'] == channel]\n",
    "    min_views = selected_channel_df['view_count'].min()\n",
    "    max_views = selected_channel_df['view_count'].max()\n",
    "    avg_views = selected_channel_df['view_count'].mean()\n",
    "    \n",
    "    min_likes = selected_channel_df['like_count'].min()\n",
    "    max_likes = selected_channel_df['like_count'].max()\n",
    "    avg_likes = selected_channel_df['like_count'].mean()\n",
    "    \n",
    "    min_comments = selected_channel_df['comment_count'].min()\n",
    "    max_comments = selected_channel_df['comment_count'].max()\n",
    "    avg_comments = selected_channel_df['comment_count'].mean()\n",
    "    \n",
    "    min_views_lst.append(min_views)\n",
    "    max_views_lst.append(max_views)\n",
    "    avg_views_lst.append(avg_views)\n",
    "    \n",
    "    min_likes_lst.append(min_likes)\n",
    "    max_likes_lst.append(max_likes)\n",
    "    avg_likes_lst.append(avg_likes)\n",
    "    \n",
    "    min_comments_lst.append(min_comments)\n",
    "    max_comments_lst.append(max_comments)\n",
    "    avg_comments_lst.append(avg_comments)\n",
    "    \n",
    "topic_2['min_views'] = min_views_lst\n",
    "topic_2['max_views'] = max_views_lst\n",
    "topic_2['avg_views'] = avg_views_lst\n",
    "\n",
    "topic_2['min_likes'] = min_likes_lst\n",
    "topic_2['max_likes'] = max_likes_lst\n",
    "topic_2['avg_likes'] = avg_likes_lst\n",
    "\n",
    "topic_2['min_comments'] = min_comments_lst\n",
    "topic_2['max_comments'] = max_comments_lst\n",
    "topic_2['avg_comments'] = avg_comments_lst\n",
    "\n",
    "min_views_lst = []\n",
    "max_views_lst = []\n",
    "avg_views_lst = []\n",
    "\n",
    "min_likes_lst = []\n",
    "max_likes_lst = []\n",
    "avg_likes_lst = []\n",
    "\n",
    "min_comments_lst = []\n",
    "max_comments_lst = []\n",
    "avg_comments_lst = []\n",
    "# Topic 3\n",
    "print(len(topic_3_channel_id))\n",
    "for channel in topic_3_channel_id:\n",
    "    selected_channel_df = videos_w_topics.loc[videos_w_topics['channel_id'] == channel]\n",
    "    min_views = selected_channel_df['view_count'].min()\n",
    "    max_views = selected_channel_df['view_count'].max()\n",
    "    avg_views = selected_channel_df['view_count'].mean()\n",
    "    \n",
    "    min_likes = selected_channel_df['like_count'].min()\n",
    "    max_likes = selected_channel_df['like_count'].max()\n",
    "    avg_likes = selected_channel_df['like_count'].mean()\n",
    "    \n",
    "    min_comments = selected_channel_df['comment_count'].min()\n",
    "    max_comments = selected_channel_df['comment_count'].max()\n",
    "    avg_comments = selected_channel_df['comment_count'].mean()\n",
    "    \n",
    "    min_views_lst.append(min_views)\n",
    "    max_views_lst.append(max_views)\n",
    "    avg_views_lst.append(avg_views)\n",
    "    \n",
    "    min_likes_lst.append(min_likes)\n",
    "    max_likes_lst.append(max_likes)\n",
    "    avg_likes_lst.append(avg_likes)\n",
    "    \n",
    "    min_comments_lst.append(min_comments)\n",
    "    max_comments_lst.append(max_comments)\n",
    "    avg_comments_lst.append(avg_comments)\n",
    "    \n",
    "topic_3['min_views'] = min_views_lst\n",
    "topic_3['max_views'] = max_views_lst\n",
    "topic_3['avg_views'] = avg_views_lst\n",
    "\n",
    "topic_3['min_likes'] = min_likes_lst\n",
    "topic_3['max_likes'] = max_likes_lst\n",
    "topic_3['avg_likes'] = avg_likes_lst\n",
    "\n",
    "topic_3['min_comments'] = min_comments_lst\n",
    "topic_3['max_comments'] = max_comments_lst\n",
    "topic_3['avg_comments'] = avg_comments_lst\n",
    "\n",
    "min_views_lst = []\n",
    "max_views_lst = []\n",
    "avg_views_lst = []\n",
    "\n",
    "min_likes_lst = []\n",
    "max_likes_lst = []\n",
    "avg_likes_lst = []\n",
    "\n",
    "min_comments_lst = []\n",
    "max_comments_lst = []\n",
    "avg_comments_lst = []\n",
    "#Topic 4\n",
    "print(len(topic_4_channel_id))\n",
    "for channel in topic_4_channel_id:\n",
    "    selected_channel_df = videos_w_topics.loc[videos_w_topics['channel_id'] == channel]\n",
    "    min_views = selected_channel_df['view_count'].min()\n",
    "    max_views = selected_channel_df['view_count'].max()\n",
    "    avg_views = selected_channel_df['view_count'].mean()\n",
    "    \n",
    "    min_likes = selected_channel_df['like_count'].min()\n",
    "    max_likes = selected_channel_df['like_count'].max()\n",
    "    avg_likes = selected_channel_df['like_count'].mean()\n",
    "    \n",
    "    min_comments = selected_channel_df['comment_count'].min()\n",
    "    max_comments = selected_channel_df['comment_count'].max()\n",
    "    avg_comments = selected_channel_df['comment_count'].mean()\n",
    "    \n",
    "    min_views_lst.append(min_views)\n",
    "    max_views_lst.append(max_views)\n",
    "    avg_views_lst.append(avg_views)\n",
    "    \n",
    "    min_likes_lst.append(min_likes)\n",
    "    max_likes_lst.append(max_likes)\n",
    "    avg_likes_lst.append(avg_likes)\n",
    "    \n",
    "    min_comments_lst.append(min_comments)\n",
    "    max_comments_lst.append(max_comments)\n",
    "    avg_comments_lst.append(avg_comments)\n",
    "    \n",
    "topic_4['min_views'] = min_views_lst\n",
    "topic_4['max_views'] = max_views_lst\n",
    "topic_4['avg_views'] = avg_views_lst\n",
    "\n",
    "topic_4['min_likes'] = min_likes_lst\n",
    "topic_4['max_likes'] = max_likes_lst\n",
    "topic_4['avg_likes'] = avg_likes_lst\n",
    "\n",
    "topic_4['min_comments'] = min_comments_lst\n",
    "topic_4['max_comments'] = max_comments_lst\n",
    "topic_4['avg_comments'] = avg_comments_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "8913ad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     min_views  max_views  avg_views  min_likes  max_likes  avg_likes  \\\n",
      "0     0.000179   0.000224   0.000305   0.000094   0.000199   0.000202   \n",
      "1     0.000612   0.000596   0.000880   0.000790   0.000934   0.001057   \n",
      "2     0.084929   0.042101   0.075379   0.071671   0.043404   0.064378   \n",
      "3     0.012772   0.014775   0.018976   0.022513   0.020850   0.025818   \n",
      "4     0.002565   0.004948   0.005887   0.000475   0.002836   0.002138   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "107   0.005131   0.014056   0.010405   0.002691   0.011181   0.005704   \n",
      "108   0.000289   0.006743   0.002554   0.000381   0.003149   0.001256   \n",
      "109   0.000703   0.007105   0.003340   0.000378   0.002430   0.001207   \n",
      "110   0.043779   0.022502   0.039425   0.075312   0.043576   0.066865   \n",
      "111   0.001071   0.025483   0.008336   0.000583   0.008892   0.002847   \n",
      "\n",
      "     min_comments  max_comments  avg_comments  \n",
      "0        0.001358      0.000577      0.001147  \n",
      "1        0.001619      0.002262      0.002669  \n",
      "2        0.056837      0.030547      0.055106  \n",
      "3        0.014908      0.012258      0.019194  \n",
      "4        0.003133      0.012658      0.010780  \n",
      "..            ...           ...           ...  \n",
      "107      0.003133      0.010480      0.007940  \n",
      "108      0.001358      0.005156      0.002667  \n",
      "109      0.001358      0.004691      0.002533  \n",
      "110      0.062425      0.022506      0.048376  \n",
      "111      0.001566      0.007977      0.004507  \n",
      "\n",
      "[112 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "topic_1_array = topic_1[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "topic_2_array = topic_2[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "topic_3_array = topic_3[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "topic_4_array = topic_4[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "\n",
    "# Step 1: Calculate the normalized decision matrix\n",
    "\n",
    "# Square all values in column\n",
    "topic_1_square = np.square(topic_1_array)\n",
    "topic_2_square = np.square(topic_2_array)\n",
    "topic_3_square = np.square(topic_3_array)\n",
    "topic_4_square = np.square(topic_4_array)\n",
    "\n",
    "# Get the sum of squares and then take the squareroute of it\n",
    "t1_sqrt_sum_0 = math.sqrt(np.sum(topic_1_square[:, 0]))\n",
    "t1_sqrt_sum_1 = math.sqrt(np.sum(topic_1_square[:, 1]))\n",
    "t1_sqrt_sum_2 = math.sqrt(np.sum(topic_1_square[:, 2]))\n",
    "t1_sqrt_sum_3 = math.sqrt(np.sum(topic_1_square[:, 3]))\n",
    "t1_sqrt_sum_4 = math.sqrt(np.sum(topic_1_square[:, 4]))\n",
    "t1_sqrt_sum_5 = math.sqrt(np.sum(topic_1_square[:, 5]))\n",
    "t1_sqrt_sum_6 = math.sqrt(np.sum(topic_1_square[:, 6]))\n",
    "t1_sqrt_sum_7 = math.sqrt(np.sum(topic_1_square[:, 7]))\n",
    "t1_sqrt_sum_8 = math.sqrt(np.sum(topic_1_square[:, 8]))\n",
    "\n",
    "t2_sqrt_sum_0 = math.sqrt(np.sum(topic_2_square[:, 0]))\n",
    "t2_sqrt_sum_1 = math.sqrt(np.sum(topic_2_square[:, 1]))\n",
    "t2_sqrt_sum_2 = math.sqrt(np.sum(topic_2_square[:, 2]))\n",
    "t2_sqrt_sum_3 = math.sqrt(np.sum(topic_2_square[:, 3]))\n",
    "t2_sqrt_sum_4 = math.sqrt(np.sum(topic_2_square[:, 4]))\n",
    "t2_sqrt_sum_5 = math.sqrt(np.sum(topic_2_square[:, 5]))\n",
    "t2_sqrt_sum_6 = math.sqrt(np.sum(topic_2_square[:, 6]))\n",
    "t2_sqrt_sum_7 = math.sqrt(np.sum(topic_2_square[:, 7]))\n",
    "t2_sqrt_sum_8 = math.sqrt(np.sum(topic_2_square[:, 8]))\n",
    "\n",
    "t3_sqrt_sum_0 = math.sqrt(np.sum(topic_3_square[:, 0]))\n",
    "t3_sqrt_sum_1 = math.sqrt(np.sum(topic_3_square[:, 1]))\n",
    "t3_sqrt_sum_2 = math.sqrt(np.sum(topic_3_square[:, 2]))\n",
    "t3_sqrt_sum_3 = math.sqrt(np.sum(topic_3_square[:, 3]))\n",
    "t3_sqrt_sum_4 = math.sqrt(np.sum(topic_3_square[:, 4]))\n",
    "t3_sqrt_sum_5 = math.sqrt(np.sum(topic_3_square[:, 5]))\n",
    "t3_sqrt_sum_6 = math.sqrt(np.sum(topic_3_square[:, 6]))\n",
    "t3_sqrt_sum_7 = math.sqrt(np.sum(topic_3_square[:, 7]))\n",
    "t3_sqrt_sum_8 = math.sqrt(np.sum(topic_3_square[:, 8]))\n",
    "\n",
    "t4_sqrt_sum_0 = math.sqrt(np.sum(topic_4_square[:, 0]))\n",
    "t4_sqrt_sum_1 = math.sqrt(np.sum(topic_4_square[:, 1]))\n",
    "t4_sqrt_sum_2 = math.sqrt(np.sum(topic_4_square[:, 2]))\n",
    "t4_sqrt_sum_3 = math.sqrt(np.sum(topic_4_square[:, 3]))\n",
    "t4_sqrt_sum_4 = math.sqrt(np.sum(topic_4_square[:, 4]))\n",
    "t4_sqrt_sum_5 = math.sqrt(np.sum(topic_4_square[:, 5]))\n",
    "t4_sqrt_sum_6 = math.sqrt(np.sum(topic_4_square[:, 6]))\n",
    "t4_sqrt_sum_7 = math.sqrt(np.sum(topic_4_square[:, 7]))\n",
    "t4_sqrt_sum_8 = math.sqrt(np.sum(topic_4_square[:, 8]))\n",
    "\n",
    "t1_column_0 = topic_1_array[:, 0]\n",
    "t1_column_1 = topic_1_array[:, 1]\n",
    "t1_column_2 = topic_1_array[:, 2]\n",
    "t1_column_3 = topic_1_array[:, 3]\n",
    "t1_column_4 = topic_1_array[:, 4]\n",
    "t1_column_5 = topic_1_array[:, 5]\n",
    "t1_column_6 = topic_1_array[:, 6]\n",
    "t1_column_7 = topic_1_array[:, 7]\n",
    "t1_column_8 = topic_1_array[:, 8]\n",
    "\n",
    "t2_column_0 = topic_2_array[:, 0]\n",
    "t2_column_1 = topic_2_array[:, 1]\n",
    "t2_column_2 = topic_2_array[:, 2]\n",
    "t2_column_3 = topic_2_array[:, 3]\n",
    "t2_column_4 = topic_2_array[:, 4]\n",
    "t2_column_5 = topic_2_array[:, 5]\n",
    "t2_column_6 = topic_2_array[:, 6]\n",
    "t2_column_7 = topic_2_array[:, 7]\n",
    "t2_column_8 = topic_2_array[:, 8]\n",
    "\n",
    "t3_column_0 = topic_3_array[:, 0]\n",
    "t3_column_1 = topic_3_array[:, 1]\n",
    "t3_column_2 = topic_3_array[:, 2]\n",
    "t3_column_3 = topic_3_array[:, 3]\n",
    "t3_column_4 = topic_3_array[:, 4]\n",
    "t3_column_5 = topic_3_array[:, 5]\n",
    "t3_column_6 = topic_3_array[:, 6]\n",
    "t3_column_7 = topic_3_array[:, 7]\n",
    "t3_column_8 = topic_3_array[:, 8]\n",
    "\n",
    "t4_column_0 = topic_4_array[:, 0]\n",
    "t4_column_1 = topic_4_array[:, 1]\n",
    "t4_column_2 = topic_4_array[:, 2]\n",
    "t4_column_3 = topic_4_array[:, 3]\n",
    "t4_column_4 = topic_4_array[:, 4]\n",
    "t4_column_5 = topic_4_array[:, 5]\n",
    "t4_column_6 = topic_4_array[:, 6]\n",
    "t4_column_7 = topic_4_array[:, 7]\n",
    "t4_column_8 = topic_4_array[:, 8]\n",
    "\n",
    "# Normalize by taking each row value and dividing it by the before calculated squareroot sum\n",
    "f = lambda x: x/sqrt_sum\n",
    "\n",
    "sqrt_sum = t1_sqrt_sum_0\n",
    "t1_nor_column_0 = f(t1_column_0)\n",
    "sqrt_sum = t1_sqrt_sum_1\n",
    "t1_nor_column_1 = f(t1_column_1)\n",
    "sqrt_sum = t1_sqrt_sum_2\n",
    "t1_nor_column_2 = f(t1_column_2)\n",
    "sqrt_sum = t1_sqrt_sum_3\n",
    "t1_nor_column_3 = f(t1_column_3)\n",
    "sqrt_sum = t1_sqrt_sum_4\n",
    "t1_nor_column_4 = f(t1_column_4)\n",
    "sqrt_sum = t1_sqrt_sum_5\n",
    "t1_nor_column_5 = f(t1_column_5)\n",
    "sqrt_sum = t1_sqrt_sum_6\n",
    "t1_nor_column_6 = f(t1_column_6)\n",
    "sqrt_sum = t1_sqrt_sum_7\n",
    "t1_nor_column_7 = f(t1_column_7)\n",
    "sqrt_sum = t1_sqrt_sum_8\n",
    "t1_nor_column_8 = f(t1_column_8)\n",
    "\n",
    "sqrt_sum = t2_sqrt_sum_0\n",
    "t2_nor_column_0 = f(t2_column_0)\n",
    "sqrt_sum = t2_sqrt_sum_1\n",
    "t2_nor_column_1 = f(t2_column_1)\n",
    "sqrt_sum = t2_sqrt_sum_2\n",
    "t2_nor_column_2 = f(t2_column_2)\n",
    "sqrt_sum = t2_sqrt_sum_3\n",
    "t2_nor_column_3 = f(t2_column_3)\n",
    "sqrt_sum = t2_sqrt_sum_4\n",
    "t2_nor_column_4 = f(t2_column_4)\n",
    "sqrt_sum = t2_sqrt_sum_5\n",
    "t2_nor_column_5 = f(t2_column_5)\n",
    "sqrt_sum = t2_sqrt_sum_6\n",
    "t2_nor_column_6 = f(t2_column_6)\n",
    "sqrt_sum = t2_sqrt_sum_7\n",
    "t2_nor_column_7 = f(t2_column_7)\n",
    "sqrt_sum = t2_sqrt_sum_8\n",
    "t2_nor_column_8 = f(t2_column_8)\n",
    "\n",
    "sqrt_sum = t3_sqrt_sum_0\n",
    "t3_nor_column_0 = f(t3_column_0)\n",
    "sqrt_sum = t3_sqrt_sum_1\n",
    "t3_nor_column_1 = f(t3_column_1)\n",
    "sqrt_sum = t3_sqrt_sum_2\n",
    "t3_nor_column_2 = f(t3_column_2)\n",
    "sqrt_sum = t3_sqrt_sum_3\n",
    "t3_nor_column_3 = f(t3_column_3)\n",
    "sqrt_sum = t3_sqrt_sum_4\n",
    "t3_nor_column_4 = f(t3_column_4)\n",
    "sqrt_sum = t3_sqrt_sum_5\n",
    "t3_nor_column_5 = f(t3_column_5)\n",
    "sqrt_sum = t3_sqrt_sum_6\n",
    "t3_nor_column_6 = f(t3_column_6)\n",
    "sqrt_sum = t3_sqrt_sum_7\n",
    "t3_nor_column_7 = f(t3_column_7)\n",
    "sqrt_sum = t3_sqrt_sum_8\n",
    "t3_nor_column_8 = f(t3_column_8)\n",
    "\n",
    "sqrt_sum = t4_sqrt_sum_0\n",
    "t4_nor_column_0 = f(t4_column_0)\n",
    "sqrt_sum = t4_sqrt_sum_1\n",
    "t4_nor_column_1 = f(t4_column_1)\n",
    "sqrt_sum = t4_sqrt_sum_2\n",
    "t4_nor_column_2 = f(t4_column_2)\n",
    "sqrt_sum = t4_sqrt_sum_3\n",
    "t4_nor_column_3 = f(t4_column_3)\n",
    "sqrt_sum = t4_sqrt_sum_4\n",
    "t4_nor_column_4 = f(t4_column_4)\n",
    "sqrt_sum = t4_sqrt_sum_5\n",
    "t4_nor_column_5 = f(t4_column_5)\n",
    "sqrt_sum = t4_sqrt_sum_6\n",
    "t4_nor_column_6 = f(t4_column_6)\n",
    "sqrt_sum = t4_sqrt_sum_7\n",
    "t4_nor_column_7 = f(t4_column_7)\n",
    "sqrt_sum = t4_sqrt_sum_8\n",
    "t4_nor_column_8 = f(t4_column_8)\n",
    "\n",
    "# Step 2: Calculate the weighted decision matrix\n",
    "# Weight of 1/9 because of 9 attributes and Xiao etal. 2015: \"We suppose each metric has the same weight\"\n",
    "weight = 1/9\n",
    "\n",
    "weight_function = lambda x: x*weight\n",
    "t1_weight_column_0 = weight_function(t1_nor_column_0)\n",
    "t1_weight_column_1 = weight_function(t1_nor_column_1)\n",
    "t1_weight_column_2 = weight_function(t1_nor_column_2)\n",
    "t1_weight_column_3 = weight_function(t1_nor_column_3)\n",
    "t1_weight_column_4 = weight_function(t1_nor_column_4)\n",
    "t1_weight_column_5 = weight_function(t1_nor_column_5)\n",
    "t1_weight_column_6 = weight_function(t1_nor_column_6)\n",
    "t1_weight_column_7 = weight_function(t1_nor_column_7)\n",
    "t1_weight_column_8 = weight_function(t1_nor_column_8)\n",
    "\n",
    "t2_weight_column_0 = weight_function(t2_nor_column_0)\n",
    "t2_weight_column_1 = weight_function(t2_nor_column_1)\n",
    "t2_weight_column_2 = weight_function(t2_nor_column_2)\n",
    "t2_weight_column_3 = weight_function(t2_nor_column_3)\n",
    "t2_weight_column_4 = weight_function(t2_nor_column_4)\n",
    "t2_weight_column_5 = weight_function(t2_nor_column_5)\n",
    "t2_weight_column_6 = weight_function(t2_nor_column_6)\n",
    "t2_weight_column_7 = weight_function(t2_nor_column_7)\n",
    "t2_weight_column_8 = weight_function(t2_nor_column_8)\n",
    "\n",
    "t3_weight_column_0 = weight_function(t3_nor_column_0)\n",
    "t3_weight_column_1 = weight_function(t3_nor_column_1)\n",
    "t3_weight_column_2 = weight_function(t3_nor_column_2)\n",
    "t3_weight_column_3 = weight_function(t3_nor_column_3)\n",
    "t3_weight_column_4 = weight_function(t3_nor_column_4)\n",
    "t3_weight_column_5 = weight_function(t3_nor_column_5)\n",
    "t3_weight_column_6 = weight_function(t3_nor_column_6)\n",
    "t3_weight_column_7 = weight_function(t3_nor_column_7)\n",
    "t3_weight_column_8 = weight_function(t3_nor_column_8)\n",
    "\n",
    "t4_weight_column_0 = weight_function(t4_nor_column_0)\n",
    "t4_weight_column_1 = weight_function(t4_nor_column_1)\n",
    "t4_weight_column_2 = weight_function(t4_nor_column_2)\n",
    "t4_weight_column_3 = weight_function(t4_nor_column_3)\n",
    "t4_weight_column_4 = weight_function(t4_nor_column_4)\n",
    "t4_weight_column_5 = weight_function(t4_nor_column_5)\n",
    "t4_weight_column_6 = weight_function(t4_nor_column_6)\n",
    "t4_weight_column_7 = weight_function(t4_nor_column_7)\n",
    "t4_weight_column_8 = weight_function(t4_nor_column_8)\n",
    "\n",
    "# Step 3: Determine the ideal and negative-ideal solutions:\n",
    "t1_max_column_0 = np.max(t1_weight_column_0)\n",
    "t1_max_column_1 = np.max(t1_weight_column_1)\n",
    "t1_max_column_2 = np.max(t1_weight_column_2)\n",
    "t1_max_column_3 = np.max(t1_weight_column_3)\n",
    "t1_max_column_4 = np.max(t1_weight_column_4)\n",
    "t1_max_column_5 = np.max(t1_weight_column_5)\n",
    "t1_max_column_6 = np.max(t1_weight_column_6)\n",
    "t1_max_column_7 = np.max(t1_weight_column_7)\n",
    "t1_max_column_8 = np.max(t1_weight_column_8)\n",
    "\n",
    "t2_max_column_0 = np.max(t2_weight_column_0)\n",
    "t2_max_column_1 = np.max(t2_weight_column_1)\n",
    "t2_max_column_2 = np.max(t2_weight_column_2)\n",
    "t2_max_column_3 = np.max(t2_weight_column_3)\n",
    "t2_max_column_4 = np.max(t2_weight_column_4)\n",
    "t2_max_column_5 = np.max(t2_weight_column_5)\n",
    "t2_max_column_6 = np.max(t2_weight_column_6)\n",
    "t2_max_column_7 = np.max(t2_weight_column_7)\n",
    "t2_max_column_8 = np.max(t2_weight_column_8)\n",
    "\n",
    "t3_max_column_0 = np.max(t3_weight_column_0)\n",
    "t3_max_column_1 = np.max(t3_weight_column_1)\n",
    "t3_max_column_2 = np.max(t3_weight_column_2)\n",
    "t3_max_column_3 = np.max(t3_weight_column_3)\n",
    "t3_max_column_4 = np.max(t3_weight_column_4)\n",
    "t3_max_column_5 = np.max(t3_weight_column_5)\n",
    "t3_max_column_6 = np.max(t3_weight_column_6)\n",
    "t3_max_column_7 = np.max(t3_weight_column_7)\n",
    "t3_max_column_8 = np.max(t3_weight_column_8)\n",
    "\n",
    "t4_max_column_0 = np.max(t4_weight_column_0)\n",
    "t4_max_column_1 = np.max(t4_weight_column_1)\n",
    "t4_max_column_2 = np.max(t4_weight_column_2)\n",
    "t4_max_column_3 = np.max(t4_weight_column_3)\n",
    "t4_max_column_4 = np.max(t4_weight_column_4)\n",
    "t4_max_column_5 = np.max(t4_weight_column_5)\n",
    "t4_max_column_6 = np.max(t4_weight_column_6)\n",
    "t4_max_column_7 = np.max(t4_weight_column_7)\n",
    "t4_max_column_8 = np.max(t4_weight_column_8)\n",
    "\n",
    "t1_min_column_0 = np.min(t1_weight_column_0)\n",
    "t1_min_column_1 = np.min(t1_weight_column_1)\n",
    "t1_min_column_2 = np.min(t1_weight_column_2)\n",
    "t1_min_column_3 = np.min(t1_weight_column_3)\n",
    "t1_min_column_4 = np.min(t1_weight_column_4)\n",
    "t1_min_column_5 = np.min(t1_weight_column_5)\n",
    "t1_min_column_6 = np.min(t1_weight_column_6)\n",
    "t1_min_column_7 = np.min(t1_weight_column_7)\n",
    "t1_min_column_8 = np.min(t1_weight_column_8)\n",
    "\n",
    "t2_min_column_0 = np.min(t2_weight_column_0)\n",
    "t2_min_column_1 = np.min(t2_weight_column_1)\n",
    "t2_min_column_2 = np.min(t2_weight_column_2)\n",
    "t2_min_column_3 = np.min(t2_weight_column_3)\n",
    "t2_min_column_4 = np.min(t2_weight_column_4)\n",
    "t2_min_column_5 = np.min(t2_weight_column_5)\n",
    "t2_min_column_6 = np.min(t2_weight_column_6)\n",
    "t2_min_column_7 = np.min(t2_weight_column_7)\n",
    "t2_min_column_8 = np.min(t2_weight_column_8)\n",
    "\n",
    "t3_min_column_0 = np.min(t3_weight_column_0)\n",
    "t3_min_column_1 = np.min(t3_weight_column_1)\n",
    "t3_min_column_2 = np.min(t3_weight_column_2)\n",
    "t3_min_column_3 = np.min(t3_weight_column_3)\n",
    "t3_min_column_4 = np.min(t3_weight_column_4)\n",
    "t3_min_column_5 = np.min(t3_weight_column_5)\n",
    "t3_min_column_6 = np.min(t3_weight_column_6)\n",
    "t3_min_column_7 = np.min(t3_weight_column_7)\n",
    "t3_min_column_8 = np.min(t3_weight_column_8)\n",
    "\n",
    "t4_min_column_0 = np.min(t4_weight_column_0)\n",
    "t4_min_column_1 = np.min(t4_weight_column_1)\n",
    "t4_min_column_2 = np.min(t4_weight_column_2)\n",
    "t4_min_column_3 = np.min(t4_weight_column_3)\n",
    "t4_min_column_4 = np.min(t4_weight_column_4)\n",
    "t4_min_column_5 = np.min(t4_weight_column_5)\n",
    "t4_min_column_6 = np.min(t4_weight_column_6)\n",
    "t4_min_column_7 = np.min(t4_weight_column_7)\n",
    "t4_min_column_8 = np.min(t4_weight_column_8)\n",
    "\n",
    "t1_ideal_max = (t1_max_column_0, t1_max_column_1, t1_max_column_2, t1_max_column_3, t1_max_column_4, t1_max_column_5, t1_max_column_6, t1_max_column_7, t1_max_column_8)\n",
    "t1_ideal_min = (t1_min_column_0, t1_min_column_1, t1_min_column_2, t1_min_column_3, t1_min_column_4, t1_min_column_5, t1_min_column_6, t1_min_column_7, t1_min_column_8)\n",
    "\n",
    "t2_ideal_max = (t2_max_column_0, t2_max_column_1, t2_max_column_2, t2_max_column_3, t2_max_column_4, t2_max_column_5, t2_max_column_6, t2_max_column_7, t2_max_column_8)\n",
    "t2_ideal_min = (t2_min_column_0, t2_min_column_1, t2_min_column_2, t2_min_column_3, t2_min_column_4, t2_min_column_5, t2_min_column_6, t2_min_column_7, t2_min_column_8)\n",
    "\n",
    "t3_ideal_max = (t3_max_column_0, t3_max_column_1, t3_max_column_2, t3_max_column_3, t3_max_column_4, t3_max_column_5, t3_max_column_6, t3_max_column_7, t3_max_column_8)\n",
    "t3_ideal_min = (t3_min_column_0, t3_min_column_1, t3_min_column_2, t3_min_column_3, t3_min_column_4, t3_min_column_5, t3_min_column_6, t3_min_column_7, t3_min_column_8)\n",
    "\n",
    "t4_ideal_max = (t4_max_column_0, t4_max_column_1, t4_max_column_2, t4_max_column_3, t4_max_column_4, t4_max_column_5, t4_max_column_6, t4_max_column_7, t4_max_column_8)\n",
    "t4_ideal_min = (t4_min_column_0, t4_min_column_1, t4_min_column_2, t4_min_column_3, t4_min_column_4, t4_min_column_5, t4_min_column_6, t4_min_column_7, t4_min_column_8)\n",
    "# Step 4: Calculate the separation measures \n",
    "\n",
    "t1_weighted_df = pd.DataFrame({'min_views': t1_weight_column_0, 'max_views': t1_weight_column_1, 'avg_views': t1_weight_column_2, 'min_likes': t1_weight_column_3, 'max_likes': t1_weight_column_4, 'avg_likes': t1_weight_column_5, 'min_comments': t1_weight_column_6, 'max_comments': t1_weight_column_7, 'avg_comments': t1_weight_column_8})\n",
    "t2_weighted_df = pd.DataFrame({'min_views': t2_weight_column_0, 'max_views': t2_weight_column_1, 'avg_views': t2_weight_column_2, 'min_likes': t2_weight_column_3, 'max_likes': t2_weight_column_4, 'avg_likes': t2_weight_column_5, 'min_comments': t2_weight_column_6, 'max_comments': t2_weight_column_7, 'avg_comments': t2_weight_column_8})\n",
    "t3_weighted_df = pd.DataFrame({'min_views': t3_weight_column_0, 'max_views': t3_weight_column_1, 'avg_views': t3_weight_column_2, 'min_likes': t3_weight_column_3, 'max_likes': t3_weight_column_4, 'avg_likes': t3_weight_column_5, 'min_comments': t3_weight_column_6, 'max_comments': t3_weight_column_7, 'avg_comments': t3_weight_column_8})\n",
    "t4_weighted_df = pd.DataFrame({'min_views': t4_weight_column_0, 'max_views': t4_weight_column_1, 'avg_views': t4_weight_column_2, 'min_likes': t4_weight_column_3, 'max_likes': t4_weight_column_4, 'avg_likes': t4_weight_column_5, 'min_comments': t4_weight_column_6, 'max_comments': t4_weight_column_7, 'avg_comments': t4_weight_column_8})\n",
    "\n",
    "print(t1_weighted_df)\n",
    "\n",
    "#weighted_array = weighted_df[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "\n",
    "#print(weighted_array)\n",
    "\n",
    "#euc_pos = lambda x: distance.euclidean(ideal_max, x)\n",
    "\n",
    "#ideal_pos_array = euc_pos(weighted_array)\n",
    "\n",
    "#print(ideal_pos_array)\n",
    "def euclidian_max(a, b, c, d, e, f, g, h, i):\n",
    "    row_vector = (a, b, c, d, e, f, g, h, i)\n",
    "    return distance.euclidean(row_vector, ideal_max)\n",
    "\n",
    "def euclidian_min(a, b, c, d, e, f, g, h, i):\n",
    "    row_vector = (a, b, c, d, e, f, g, h, i)\n",
    "    return distance.euclidean(row_vector, ideal_min)\n",
    "\n",
    "\n",
    "t1_weighted_df['euc_pos'] = t1_weighted_df.apply(lambda row : euclidian_max(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "t1_weighted_df['euc_neg'] = t1_weighted_df.apply(lambda row : euclidian_min(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "\n",
    "\n",
    "t2_weighted_df['euc_pos'] = t2_weighted_df.apply(lambda row : euclidian_max(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "t2_weighted_df['euc_neg'] = t2_weighted_df.apply(lambda row : euclidian_min(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "\n",
    "\n",
    "t3_weighted_df['euc_pos'] = t3_weighted_df.apply(lambda row : euclidian_max(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "t3_weighted_df['euc_neg'] = t3_weighted_df.apply(lambda row : euclidian_min(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "\n",
    "\n",
    "t4_weighted_df['euc_pos'] = t4_weighted_df.apply(lambda row : euclidian_max(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "t4_weighted_df['euc_neg'] = t4_weighted_df.apply(lambda row : euclidian_min(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "\n",
    "# Step 5: Calculate the relative closeness to the ideal solution:\n",
    "def relative_closeness(pos, neg):\n",
    "    relative = neg/(pos+neg)\n",
    "    \n",
    "    return relative\n",
    "\n",
    "t1_weighted_df['euc_rel'] = t1_weighted_df.apply(lambda row : relative_closeness(row['euc_pos'],\n",
    "                     row['euc_neg']), axis = 1)\n",
    "\n",
    "t2_weighted_df['euc_rel'] = t2_weighted_df.apply(lambda row : relative_closeness(row['euc_pos'],\n",
    "                     row['euc_neg']), axis = 1)\n",
    "\n",
    "t3_weighted_df['euc_rel'] = t3_weighted_df.apply(lambda row : relative_closeness(row['euc_pos'],\n",
    "                     row['euc_neg']), axis = 1)\n",
    "\n",
    "t4_weighted_df['euc_rel'] = t4_weighted_df.apply(lambda row : relative_closeness(row['euc_pos'],\n",
    "                     row['euc_neg']), axis = 1)\n",
    "\n",
    "\n",
    "# Step 6: Rank the preference order:\n",
    "channel_id = topic_1['channel_id'].tolist()\n",
    "channel_title = topic_1['channel_title'].tolist()\n",
    "t1_weighted_df['channel_id'] = channel_id\n",
    "t1_weighted_df['channel_title'] = channel_title\n",
    "t1_weighted_df = t1_weighted_df.sort_values(by='euc_rel', ascending=False)\n",
    "\n",
    "channel_id = topic_2['channel_id'].tolist()\n",
    "channel_title = topic_2['channel_title'].tolist()\n",
    "t2_weighted_df['channel_id'] = channel_id\n",
    "t2_weighted_df['channel_title'] = channel_title\n",
    "t2_weighted_df = t2_weighted_df.sort_values(by='euc_rel', ascending=False)\n",
    "\n",
    "channel_id = topic_3['channel_id'].tolist()\n",
    "channel_title = topic_3['channel_title'].tolist()\n",
    "t3_weighted_df['channel_id'] = channel_id\n",
    "t3_weighted_df['channel_title'] = channel_title\n",
    "t3_weighted_df = t3_weighted_df.sort_values(by='euc_rel', ascending=False)\n",
    "\n",
    "channel_id = topic_4['channel_id'].tolist()\n",
    "channel_title = topic_4['channel_title'].tolist()\n",
    "t4_weighted_df['channel_id'] = channel_id\n",
    "t4_weighted_df['channel_title'] = channel_title\n",
    "t4_weighted_df = t4_weighted_df.sort_values(by='euc_rel', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "807e3592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "            channel\\_title &  euc\\_neg &  euc\\_pos &  euc\\_rel \\\\\n",
      "\\midrule\n",
      "           JP Performance & 0.180990 & 0.085386 & 0.679452 \\\\\n",
      "                AlexiBexi & 0.149993 & 0.110129 & 0.576627 \\\\\n",
      "                      WDR & 0.091010 & 0.156455 & 0.367770 \\\\\n",
      "               Car Maniac & 0.082522 & 0.153167 & 0.350131 \\\\\n",
      "           Spiel und Zeug & 0.067901 & 0.152475 & 0.308113 \\\\\n",
      "     auto motor und sport & 0.061778 & 0.165026 & 0.272386 \\\\\n",
      "             Breaking Lab & 0.057374 & 0.157706 & 0.266755 \\\\\n",
      "                  Felixba & 0.054842 & 0.161518 & 0.253477 \\\\\n",
      "      Felix von der Laden & 0.051364 & 0.167021 & 0.235199 \\\\\n",
      "Mercedes-Benz Deutschland & 0.048486 & 0.180769 & 0.211495 \\\\\n",
      "        Matthias Malmedie & 0.042959 & 0.174744 & 0.197330 \\\\\n",
      "          TopSpeedGermany & 0.039628 & 0.180061 & 0.180382 \\\\\n",
      "     ZDFheute Nachrichten & 0.039998 & 0.185937 & 0.177035 \\\\\n",
      "                 nextmove & 0.037530 & 0.179515 & 0.172915 \\\\\n",
      "                AUTO BILD & 0.035595 & 0.184596 & 0.161655 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\4253711098.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t1_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\4253711098.py:6: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(t1_head.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "t1_head = t1_weighted_df.head(15)\n",
    "t1_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
    "\n",
    "t1_head = t1_head[['channel_title', 'euc_neg', 'euc_pos', 'euc_rel']]\n",
    "\n",
    "print(t1_head.to_latex(index=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "30aaa339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "                       channel\\_title &  euc\\_neg &  euc\\_pos &  euc\\_rel \\\\\n",
      "\\midrule\n",
      "                                BR24 & 0.178313 & 0.118445 & 0.600870 \\\\\n",
      "                        Maeximiliano & 0.116963 & 0.137015 & 0.460525 \\\\\n",
      "Die Autodoktoren - offizieller Kanal & 0.103204 & 0.135311 & 0.432694 \\\\\n",
      "                             Felixba & 0.073810 & 0.155044 & 0.322520 \\\\\n",
      "                         Johnny Hand & 0.071701 & 0.161964 & 0.306853 \\\\\n",
      "                    Dr. Dirk Spaniel & 0.073753 & 0.171665 & 0.300521 \\\\\n",
      "                        Breaking Lab & 0.065621 & 0.154961 & 0.297490 \\\\\n",
      "                 Felix von der Laden & 0.062586 & 0.162785 & 0.277703 \\\\\n",
      "                          Car Maniac & 0.058398 & 0.161999 & 0.264968 \\\\\n",
      "                                BILD & 0.053171 & 0.169438 & 0.238853 \\\\\n",
      "                          Daniel Abt & 0.052880 & 0.169803 & 0.237468 \\\\\n",
      "                auto motor und sport & 0.041617 & 0.175459 & 0.191718 \\\\\n",
      "           Mercedes-Benz Deutschland & 0.042117 & 0.185861 & 0.184743 \\\\\n",
      "        Die Autogesellschaft Dresden & 0.035490 & 0.186020 & 0.160219 \\\\\n",
      "                            nextmove & 0.031065 & 0.183483 & 0.144794 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\1517027070.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t2_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\1517027070.py:6: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(t2_head.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "t2_head = t2_weighted_df.head(15)\n",
    "t2_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
    "\n",
    "t2_head = t2_head[['channel_title', 'euc_neg', 'euc_pos', 'euc_rel']]\n",
    "\n",
    "print(t2_head.to_latex(index=False))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "163fd283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "                       channel\\_title &  euc\\_neg &  euc\\_pos &  euc\\_rel \\\\\n",
      "\\midrule\n",
      "                                BR24 & 0.191472 & 0.117085 & 0.620541 \\\\\n",
      "Die Autodoktoren - offizieller Kanal & 0.135518 & 0.124546 & 0.521095 \\\\\n",
      "                      Spiel und Zeug & 0.086054 & 0.140737 & 0.379442 \\\\\n",
      "                        Breaking Lab & 0.086870 & 0.142660 & 0.378471 \\\\\n",
      "                    Dr. Dirk Spaniel & 0.087876 & 0.163610 & 0.349427 \\\\\n",
      "                                BILD & 0.070688 & 0.159842 & 0.306631 \\\\\n",
      "                          Car Maniac & 0.065831 & 0.154448 & 0.298853 \\\\\n",
      "                ZDFheute Nachrichten & 0.072829 & 0.174320 & 0.294676 \\\\\n",
      "                            nextmove & 0.036874 & 0.178436 & 0.171259 \\\\\n",
      "                             extra 3 & 0.037366 & 0.181356 & 0.170838 \\\\\n",
      "              AfD-Fraktion Bundestag & 0.036352 & 0.184671 & 0.164473 \\\\\n",
      "                             Galileo & 0.035076 & 0.182627 & 0.161119 \\\\\n",
      "                         Jonah Plank & 0.033204 & 0.185501 & 0.151823 \\\\\n",
      "                           AUTO BILD & 0.025242 & 0.189582 & 0.117502 \\\\\n",
      "                       Thanks4Giving & 0.024512 & 0.190468 & 0.114022 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\1612287346.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t3_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\1612287346.py:6: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(t3_head.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "t3_head = t3_weighted_df.head(15)\n",
    "t3_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
    "\n",
    "t3_head = t3_head[['channel_title', 'euc_neg', 'euc_pos', 'euc_rel']]\n",
    "\n",
    "print(t3_head.to_latex(index=False))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "639b3a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "                       channel\\_title &  euc\\_neg &  euc\\_pos &  euc\\_rel \\\\\n",
      "\\midrule\n",
      "                      JP Performance & 0.179062 & 0.081400 & 0.687480 \\\\\n",
      "                           AlexiBexi & 0.146502 & 0.107520 & 0.576729 \\\\\n",
      "Die Autodoktoren - offizieller Kanal & 0.084391 & 0.137765 & 0.379874 \\\\\n",
      "                                 WDR & 0.086801 & 0.155735 & 0.357890 \\\\\n",
      "                        Maeximiliano & 0.078825 & 0.143760 & 0.354135 \\\\\n",
      "                          Car Maniac & 0.081209 & 0.152296 & 0.347783 \\\\\n",
      "                      Spiel und Zeug & 0.070752 & 0.151274 & 0.318664 \\\\\n",
      "           Mercedes-Benz Deutschland & 0.052893 & 0.179439 & 0.227662 \\\\\n",
      "                   Matthias Malmedie & 0.043061 & 0.174242 & 0.198160 \\\\\n",
      "                                BILD & 0.039858 & 0.173785 & 0.186563 \\\\\n",
      "                            Der Held & 0.039100 & 0.184676 & 0.174728 \\\\\n",
      "                           AUTO BILD & 0.038224 & 0.183538 & 0.172366 \\\\\n",
      "                            nextmove & 0.037156 & 0.179251 & 0.171695 \\\\\n",
      "                   Blaulicht-Magazin & 0.028199 & 0.187241 & 0.130891 \\\\\n",
      "                           OwnGalaxy & 0.027676 & 0.184828 & 0.130239 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\1803686158.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t4_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\1803686158.py:6: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(t4_head.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "t4_head = t4_weighted_df.head(15)\n",
    "t4_head.drop(['min_views', 'max_views', 'avg_views', 'min_likes', 'max_likes', 'avg_likes', 'min_comments', 'max_comments', 'avg_comments', 'channel_id'], axis=1, inplace=True)\n",
    "\n",
    "t4_head = t4_head[['channel_title', 'euc_neg', 'euc_pos', 'euc_rel']]\n",
    "\n",
    "print(t4_head.to_latex(index=False)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
