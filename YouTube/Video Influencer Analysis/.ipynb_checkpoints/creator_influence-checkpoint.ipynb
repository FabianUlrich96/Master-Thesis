{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c34f943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import googleapiclient.discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "edb6fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"C:/Users/fabia/OneDrive - Otto-Friedrich-Universität Bamberg/Master\\Masterarbeit/Data/Video/Labeled/\"\n",
    "\n",
    "jul_20 = pd.read_csv(work_dir + \"Juli_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "aug_20 = pd.read_csv(work_dir + \"Aug_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "sep_20 = pd.read_csv(work_dir + \"Sep_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "oct_20 = pd.read_csv(work_dir + \"Oct_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "nov_20 = pd.read_csv(work_dir + \"Nov_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "dec_20 = pd.read_csv(work_dir + \"Dec_20-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jan_21 = pd.read_csv(work_dir + \"Jan_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "feb_21 = pd.read_csv(work_dir + \"Feb_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "mar_21 = pd.read_csv(work_dir + \"March_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "apr_21 = pd.read_csv(work_dir + \"Apr_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "mai_21 = pd.read_csv(work_dir + \"Mai_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jun_21 = pd.read_csv(work_dir + \"June_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jul_21 = pd.read_csv(work_dir + \"Juli_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "aug_21 = pd.read_csv(work_dir + \"Aug_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "sep_21 = pd.read_csv(work_dir + \"September_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "oct_21 = pd.read_csv(work_dir + \"October_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "nov_21 = pd.read_csv(work_dir + \"Nov_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "dec_21 = pd.read_csv(work_dir + \"December_21-Total_with_information_labeled.csv\", index_col=[0])\n",
    "jan_22 =pd.read_csv(work_dir + \"January_22-Total_with_information_labeled.csv\", index_col=[0])\n",
    "feb_22 = pd.read_csv(work_dir + \"February_22-Total_with_information_labeled.csv\", index_col=[0])\n",
    "\n",
    "jul_20[\"month\"] = \"July\"\n",
    "aug_20[\"month\"] = \"August\"\n",
    "sep_20[\"month\"] = \"September\"\n",
    "oct_20[\"month\"] = \"October\"\n",
    "nov_20[\"month\"] = \"November\"\n",
    "dec_20[\"month\"] = \"December\"\n",
    "jan_21[\"month\"] = \"January\"\n",
    "feb_21[\"month\"] = \"February\"\n",
    "mar_21[\"month\"] = \"March\"\n",
    "apr_21[\"month\"] = \"April\"\n",
    "mai_21[\"month\"] = \"Mai\"\n",
    "jun_21[\"month\"] = \"June\"\n",
    "jul_21[\"month\"] = \"July\"\n",
    "aug_21[\"month\"] = \"August\"\n",
    "sep_21[\"month\"] = \"September\"\n",
    "oct_21[\"month\"] = \"October\"\n",
    "nov_21[\"month\"] = \"November\"\n",
    "dec_21[\"month\"] = \"December\"\n",
    "jan_22[\"month\"] = \"January\"\n",
    "feb_22[\"month\"] = \"February\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8328e800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12253, 11)\n",
      "(3783, 11)\n"
     ]
    }
   ],
   "source": [
    "og_merged_df = pd.concat([jul_20, aug_20, sep_20, oct_20, nov_20, dec_20, jan_21, feb_21, mar_21, apr_21, mai_21, jun_21, jul_21, aug_21, sep_21, oct_21, nov_21, dec_21, jan_22, feb_22], ignore_index=True)\n",
    "print(og_merged_df.shape)\n",
    "\n",
    "og_merged_df = og_merged_df.loc[og_merged_df['valid'] == \"valid\"]\n",
    "\n",
    "print(og_merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "61522c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          video_id                                              title  \\\n",
      "2      7l3fREmEiuI  Im Audi Etron von Hamburg nach Leipzig in 9 St...   \n",
      "5      cOwoQ5JTxHM  Bloch reagiert auf Tesla Model Y Reichweitente...   \n",
      "11     L60hLi8KZbk  Der große Elektroauto-Schwindel | Bessere Zeit...   \n",
      "14     i3Ovee0gi6Q  Elektroauto kostenlos aufladen mit &Charge - E...   \n",
      "15     GXG_OuYT1W4  Mit dem Elektro Hyundai schneller von Berlin n...   \n",
      "...            ...                                                ...   \n",
      "11204  TTyN4Qbq1Js  Verbrauchsvergleich Tesla Model 3 SR+ (60kWh) ...   \n",
      "11207  2GU5RBXAbmQ  TESLA: Model 3 aus Grünheide? Ein Zeitrahmen &...   \n",
      "11208  Wg5Gpi9ElvU  500 km Test - Vergleich Kosten und Zeit: TΞSLA...   \n",
      "11210  9lQTLtdJZEk        TESLA: Eine Variante des Model Y gestrichen   \n",
      "11212  LSyjq7BcVb4  Volvo C40 Test | 5 Vorteile gegenüber anderen ...   \n",
      "\n",
      "               published_at                channel_id  view_count  like_count  \\\n",
      "2      2020-07-01T09:30:00Z  UCaCaZ-vKtnMG2_FKmEePChQ      5063.0       478.0   \n",
      "5      2020-07-01T14:00:10Z  UCLINPbYQ9sy6qc-TqtBeVnw    211684.0      5571.0   \n",
      "11     2020-07-02T20:43:06Z  UCPH3ZPeqWqRVZ_ef4vOZgSw    482332.0     17206.0   \n",
      "14     2020-07-02T19:00:11Z  UCBc0Mghy-6jhMXs3T7DluMg     11051.0       300.0   \n",
      "15     2020-07-02T09:30:06Z  UCaCaZ-vKtnMG2_FKmEePChQ      8625.0       432.0   \n",
      "...                     ...                       ...         ...         ...   \n",
      "11204  2022-02-20T11:32:10Z  UCQOnq7u47LRBgWXr1Xc-XWQ      4196.0       143.0   \n",
      "11207  2022-02-05T15:59:43Z  UCpPbcfwq0wOpWrC0gBs73tQ     16111.0       858.0   \n",
      "11208  2022-02-06T13:00:11Z  UC0wANY47B2CDa1_9ByPzz_A      5288.0       147.0   \n",
      "11210  2022-02-18T16:00:11Z  UCpPbcfwq0wOpWrC0gBs73tQ     16094.0       749.0   \n",
      "11212  2022-02-22T14:30:05Z  UCDDj2GWklzZ09X7R9OtsX3Q      5237.0       295.0   \n",
      "\n",
      "       comment_count  job  id  valid     month  \n",
      "2                120    0   0  valid      July  \n",
      "5                826    0   0  valid      July  \n",
      "11              4747    0   0  valid      July  \n",
      "14                63    0   0  valid      July  \n",
      "15               127    0   0  valid      July  \n",
      "...              ...  ...  ..    ...       ...  \n",
      "11204             52    0   0  valid  February  \n",
      "11207             51    0   0  valid  February  \n",
      "11208             51    0   0  valid  February  \n",
      "11210             51    0   0  valid  February  \n",
      "11212             51    0   0  valid  February  \n",
      "\n",
      "[3783 rows x 11 columns]\n",
      "(3529, 11)\n"
     ]
    }
   ],
   "source": [
    "print(og_merged_df)\n",
    "# remove entries where channel_id only appeared once in the dataset --> channel holder cannot hold influence\n",
    "unique_removed_df = og_merged_df[og_merged_df.channel_id.duplicated(keep=False)]\n",
    "\n",
    "print(unique_removed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cb096f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2916\\4122249972.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  grouped_df = unique_removed_df.groupby(['channel_id'])[\"view_count\", \"like_count\", \"comment_count\"].apply(lambda x : x.astype(int).sum())\n"
     ]
    }
   ],
   "source": [
    "grouped_df = unique_removed_df.groupby(['channel_id'])[\"view_count\", \"like_count\", \"comment_count\"].apply(lambda x : x.astype(int).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "59c4c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          view_count  like_count  comment_count\n",
      "channel_id                                                     \n",
      "UC-k-5F2j4CUzuwkx1QLRlpA      459441       17049           4290\n",
      "UC-r2l4o0_R3eRwOHthmGAcA        7776         177            114\n",
      "UC-s__WrmBUAGRQqeZxcFxlQ      633786       21430           3199\n",
      "UC0PJSqEcY5p3NtG9o9A4RWg       47097        1982            541\n",
      "UC0cCZpU5Ioj1FOuIzj7cSsQ       67332        2776            796\n",
      "...                              ...         ...            ...\n",
      "UCyQpfuhftLvrmjxgEzVH78Q       15392          93            220\n",
      "UCyw3E91jaNVEPqirEyhMRRw       42919        1247            948\n",
      "UCzH549YlZhdhIqhtvz7XHmQ     1006081       58558           4809\n",
      "UCzZ0tUcFR7APjPKdxmgkbJA       58353        2461            252\n",
      "UCzyBlIm4NE2ch85fY3Y4cOA     1170023       13715           2464\n",
      "\n",
      "[208 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df)\n",
    "\n",
    "grouped_df.to_csv(\"grouped_df.csv\", index=True)\n",
    "\n",
    "# Getting a list of subscribers is not possible because the YouTube data api only allows you to retrieve that for your own channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "21ba9ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids_lst = grouped_df.index.tolist()\n",
    "\n",
    "chunk_lst = np.array_split(channel_ids_lst, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "23c1cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "api_connection = googleapiclient.discovery.build(api_service_name, api_version, developerKey=\"AIzaSyDVseWOuiO1diHvWYZ0-76WM0RKEWo4DgE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02824661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topicDetails for topic of channel; statistics for channel statistics, snippet for channel name;\n",
    "#channel_ids = [\"UC4t0KuLYBiXqXF27UtjLxPw\", \"UC-k-5F2j4CUzuwkx1QLRlpA\"]\n",
    "def youtube_api(channel_ids_lst):\n",
    "    channel_ids_lst = channel_ids_lst.tolist()\n",
    "    request = api_connection.channels().list(\n",
    "                        part=\"snippet, statistics, topicDetails\", id=channel_ids_lst, maxResults=50, fields=\"items()\")\n",
    "    response = request.execute()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4b1d2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_titles = []\n",
    "channel_descriptions = []\n",
    "channel_subscribers = []\n",
    "channel_topics = []\n",
    "channel_ids = []\n",
    "\n",
    "for chunk in chunk_lst:\n",
    "    response = youtube_api(chunk)\n",
    "    for item in response[\"items\"]:\n",
    "        channel_ids.append(item[\"id\"])\n",
    "        channel_titles.append(item[\"snippet\"][\"title\"])\n",
    "        channel_descriptions.append(item[\"snippet\"][\"description\"])\n",
    "        try:\n",
    "            channel_subscribers.append(item[\"statistics\"][\"subscriberCount\"])\n",
    "        except:\n",
    "            channel_subscribers.append(None)\n",
    "        channel_topics.append(item[\"topicDetails\"][\"topicCategories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ee3f18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_info_df = pd.DataFrame(\n",
    "    {'channel_id': channel_ids,\n",
    "    'channel_title': channel_titles,\n",
    "     'channel_description': channel_descriptions,\n",
    "     'channel_subscribers': channel_subscribers,\n",
    "     'channel_topic': channel_topics\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "60575b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   channel_id                 channel_title  \\\n",
      "0    UCBCT7EV5YrNLPvrhXYTHJuw       Jürs Lackiererei Lübeck   \n",
      "1    UCAszOEwa5CS4WFwYpkjdaUQ                  heise online   \n",
      "2    UCBTJnSirNI8kwOn5ROB_BGA            Autozentrum Walter   \n",
      "3    UC81R4gy-4qqoV6l6OUpDeIg                      Techning   \n",
      "4    UC96MGGyxaZZQ6aFKSVMHIew                   ECARIO.info   \n",
      "..                        ...                           ...   \n",
      "203  UCvKSljIz5MuYz0J7GnpvPHQ                   AutoScout24   \n",
      "204  UCy82YSU9FxoYhknZ4PFmjrQ                    E3/DC GmbH   \n",
      "205  UCvwE_EBBDWM552Vd9v8m9Gg  MDR Mitteldeutscher Rundfunk   \n",
      "206  UCutQkZpAs9xG0KaooZlnuTw            Motor1 Deutschland   \n",
      "207  UCwFUwxY9TUiVwr4vQ3zX_6w                    MotorWoche   \n",
      "\n",
      "                                   channel_description channel_subscribers  \\\n",
      "0    Mein Name ist Michael Scharnberg, ich bin Fahr...               14400   \n",
      "1    Unser wöchentlicher Veröffentlichungsplan:\\nMo...               82500   \n",
      "2    Unser Autozentrum Walter aus Pforzheim / Enzkr...                4380   \n",
      "3    Hi, mein Name ist Henning und ich interessiere...                 523   \n",
      "4    Bei ECARIO dreht sich alles um die Themen E-Mo...               13500   \n",
      "..                                                 ...                 ...   \n",
      "203  Wir vergleichen, testen, interviewen und stell...               24600   \n",
      "204  100% Einsatz für 100% Autarkie\\n\\nEnergie selb...               18400   \n",
      "205  Willkommen im offiziellen YouTube-Kanal des Mi...              190000   \n",
      "206  Wir machen was mit und über Autos… wie so viel...               62200   \n",
      "207  Automotive\\n\\nImpressum\\nAngaben gemäß §5 TMG ...              124000   \n",
      "\n",
      "                                         channel_topic  view_count  \\\n",
      "0    [https://en.wikipedia.org/wiki/Lifestyle_(soci...      158461   \n",
      "1              [https://en.wikipedia.org/wiki/Society]       10390   \n",
      "2    [https://en.wikipedia.org/wiki/Lifestyle_(soci...      110458   \n",
      "3    [https://en.wikipedia.org/wiki/Lifestyle_(soci...       15173   \n",
      "4    [https://en.wikipedia.org/wiki/Lifestyle_(soci...      697975   \n",
      "..                                                 ...         ...   \n",
      "203  [https://en.wikipedia.org/wiki/Vehicle, https:...      151306   \n",
      "204            [https://en.wikipedia.org/wiki/Society]       92696   \n",
      "205  [https://en.wikipedia.org/wiki/Lifestyle_(soci...       90424   \n",
      "206  [https://en.wikipedia.org/wiki/Lifestyle_(soci...      371509   \n",
      "207  [https://en.wikipedia.org/wiki/Vehicle, https:...      187881   \n",
      "\n",
      "     like_count  comment_count  \n",
      "0          9080           1795  \n",
      "1           257            171  \n",
      "2          1334            413  \n",
      "3           430            160  \n",
      "4         18178           3210  \n",
      "..          ...            ...  \n",
      "203         695            140  \n",
      "204        1355            348  \n",
      "205         595            221  \n",
      "206        8793           1508  \n",
      "207        5907            848  \n",
      "\n",
      "[208 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(channel_info_df, grouped_df, on='channel_id',  how='inner')\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3ea40d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          video_id                                              title  \\\n",
      "2      7l3fREmEiuI  Im Audi Etron von Hamburg nach Leipzig in 9 St...   \n",
      "5      cOwoQ5JTxHM  Bloch reagiert auf Tesla Model Y Reichweitente...   \n",
      "11     L60hLi8KZbk  Der große Elektroauto-Schwindel | Bessere Zeit...   \n",
      "14     i3Ovee0gi6Q  Elektroauto kostenlos aufladen mit &Charge - E...   \n",
      "15     GXG_OuYT1W4  Mit dem Elektro Hyundai schneller von Berlin n...   \n",
      "...            ...                                                ...   \n",
      "11204  TTyN4Qbq1Js  Verbrauchsvergleich Tesla Model 3 SR+ (60kWh) ...   \n",
      "11207  2GU5RBXAbmQ  TESLA: Model 3 aus Grünheide? Ein Zeitrahmen &...   \n",
      "11208  Wg5Gpi9ElvU  500 km Test - Vergleich Kosten und Zeit: TΞSLA...   \n",
      "11210  9lQTLtdJZEk        TESLA: Eine Variante des Model Y gestrichen   \n",
      "11212  LSyjq7BcVb4  Volvo C40 Test | 5 Vorteile gegenüber anderen ...   \n",
      "\n",
      "               published_at                channel_id  view_count  like_count  \\\n",
      "2      2020-07-01T09:30:00Z  UCaCaZ-vKtnMG2_FKmEePChQ        5063         478   \n",
      "5      2020-07-01T14:00:10Z  UCLINPbYQ9sy6qc-TqtBeVnw      211684        5571   \n",
      "11     2020-07-02T20:43:06Z  UCPH3ZPeqWqRVZ_ef4vOZgSw      482332       17206   \n",
      "14     2020-07-02T19:00:11Z  UCBc0Mghy-6jhMXs3T7DluMg       11051         300   \n",
      "15     2020-07-02T09:30:06Z  UCaCaZ-vKtnMG2_FKmEePChQ        8625         432   \n",
      "...                     ...                       ...         ...         ...   \n",
      "11204  2022-02-20T11:32:10Z  UCQOnq7u47LRBgWXr1Xc-XWQ        4196         143   \n",
      "11207  2022-02-05T15:59:43Z  UCpPbcfwq0wOpWrC0gBs73tQ       16111         858   \n",
      "11208  2022-02-06T13:00:11Z  UC0wANY47B2CDa1_9ByPzz_A        5288         147   \n",
      "11210  2022-02-18T16:00:11Z  UCpPbcfwq0wOpWrC0gBs73tQ       16094         749   \n",
      "11212  2022-02-22T14:30:05Z  UCDDj2GWklzZ09X7R9OtsX3Q        5237         295   \n",
      "\n",
      "      comment_count job id  valid     month  \n",
      "2               120   0  0  valid      July  \n",
      "5               826   0  0  valid      July  \n",
      "11             4747   0  0  valid      July  \n",
      "14               63   0  0  valid      July  \n",
      "15              127   0  0  valid      July  \n",
      "...             ...  .. ..    ...       ...  \n",
      "11204            52   0  0  valid  February  \n",
      "11207            51   0  0  valid  February  \n",
      "11208            51   0  0  valid  February  \n",
      "11210            51   0  0  valid  February  \n",
      "11212            51   0  0  valid  February  \n",
      "\n",
      "[3783 rows x 11 columns]\n",
      "          video_id                                              title  \\\n",
      "2      7l3fREmEiuI  Im Audi Etron von Hamburg nach Leipzig in 9 St...   \n",
      "5      cOwoQ5JTxHM  Bloch reagiert auf Tesla Model Y Reichweitente...   \n",
      "11     L60hLi8KZbk  Der große Elektroauto-Schwindel | Bessere Zeit...   \n",
      "14     i3Ovee0gi6Q  Elektroauto kostenlos aufladen mit &Charge - E...   \n",
      "15     GXG_OuYT1W4  Mit dem Elektro Hyundai schneller von Berlin n...   \n",
      "...            ...                                                ...   \n",
      "11204  TTyN4Qbq1Js  Verbrauchsvergleich Tesla Model 3 SR+ (60kWh) ...   \n",
      "11207  2GU5RBXAbmQ  TESLA: Model 3 aus Grünheide? Ein Zeitrahmen &...   \n",
      "11208  Wg5Gpi9ElvU  500 km Test - Vergleich Kosten und Zeit: TΞSLA...   \n",
      "11210  9lQTLtdJZEk        TESLA: Eine Variante des Model Y gestrichen   \n",
      "11212  LSyjq7BcVb4  Volvo C40 Test | 5 Vorteile gegenüber anderen ...   \n",
      "\n",
      "               published_at                channel_id  view_count  like_count  \\\n",
      "2      2020-07-01T09:30:00Z  UCaCaZ-vKtnMG2_FKmEePChQ        5063         478   \n",
      "5      2020-07-01T14:00:10Z  UCLINPbYQ9sy6qc-TqtBeVnw      211684        5571   \n",
      "11     2020-07-02T20:43:06Z  UCPH3ZPeqWqRVZ_ef4vOZgSw      482332       17206   \n",
      "14     2020-07-02T19:00:11Z  UCBc0Mghy-6jhMXs3T7DluMg       11051         300   \n",
      "15     2020-07-02T09:30:06Z  UCaCaZ-vKtnMG2_FKmEePChQ        8625         432   \n",
      "...                     ...                       ...         ...         ...   \n",
      "11204  2022-02-20T11:32:10Z  UCQOnq7u47LRBgWXr1Xc-XWQ        4196         143   \n",
      "11207  2022-02-05T15:59:43Z  UCpPbcfwq0wOpWrC0gBs73tQ       16111         858   \n",
      "11208  2022-02-06T13:00:11Z  UC0wANY47B2CDa1_9ByPzz_A        5288         147   \n",
      "11210  2022-02-18T16:00:11Z  UCpPbcfwq0wOpWrC0gBs73tQ       16094         749   \n",
      "11212  2022-02-22T14:30:05Z  UCDDj2GWklzZ09X7R9OtsX3Q        5237         295   \n",
      "\n",
      "      comment_count job id  valid     month  \n",
      "2               120   0  0  valid      July  \n",
      "5               826   0  0  valid      July  \n",
      "11             4747   0  0  valid      July  \n",
      "14               63   0  0  valid      July  \n",
      "15              127   0  0  valid      July  \n",
      "...             ...  .. ..    ...       ...  \n",
      "11204            52   0  0  valid  February  \n",
      "11207            51   0  0  valid  February  \n",
      "11208            51   0  0  valid  February  \n",
      "11210            51   0  0  valid  February  \n",
      "11212            51   0  0  valid  February  \n",
      "\n",
      "[3783 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(og_merged_df)\n",
    "columns = ['view_count', 'like_count']\n",
    "og_merged_df[columns] = og_merged_df[columns].fillna(0).astype(int)\n",
    "print(og_merged_df)\n",
    "\n",
    "\n",
    "unique_channels_lst = merged_df['channel_id'].tolist()\n",
    "min_views_lst = []\n",
    "max_views_lst = []\n",
    "avg_views_lst = []\n",
    "\n",
    "min_likes_lst = []\n",
    "max_likes_lst = []\n",
    "avg_likes_lst = []\n",
    "\n",
    "min_comments_lst = []\n",
    "max_comments_lst = []\n",
    "avg_comments_lst = []\n",
    "\n",
    "for channel in unique_channels_lst:\n",
    "    selected_channel_df = og_merged_df.loc[og_merged_df['channel_id'] == channel]\n",
    "    min_views = selected_channel_df['view_count'].min()\n",
    "    max_views = selected_channel_df['view_count'].max()\n",
    "    avg_views = selected_channel_df['view_count'].mean()\n",
    "    \n",
    "    min_likes = selected_channel_df['like_count'].min()\n",
    "    max_likes = selected_channel_df['like_count'].max()\n",
    "    avg_likes = selected_channel_df['like_count'].mean()\n",
    "    \n",
    "    min_comments = selected_channel_df['comment_count'].min()\n",
    "    max_comments = selected_channel_df['comment_count'].max()\n",
    "    avg_comments = selected_channel_df['comment_count'].mean()\n",
    "    \n",
    "    min_views_lst.append(min_views)\n",
    "    max_views_lst.append(max_views)\n",
    "    avg_views_lst.append(avg_views)\n",
    "    \n",
    "    min_likes_lst.append(min_likes)\n",
    "    max_likes_lst.append(max_likes)\n",
    "    avg_likes_lst.append(avg_likes)\n",
    "    \n",
    "    min_comments_lst.append(min_comments)\n",
    "    max_comments_lst.append(max_comments)\n",
    "    avg_comments_lst.append(avg_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85eb1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['min_views'] = min_views_lst\n",
    "merged_df['max_views'] = max_views_lst\n",
    "merged_df['avg_views'] = avg_views_lst\n",
    "\n",
    "merged_df['min_likes'] = min_likes_lst\n",
    "merged_df['max_likes'] = max_likes_lst\n",
    "merged_df['avg_likes'] = avg_likes_lst\n",
    "\n",
    "merged_df['min_comments'] = min_comments_lst\n",
    "merged_df['max_comments'] = max_comments_lst\n",
    "merged_df['avg_comments'] = avg_comments_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "502f733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"grouped_df_with_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3faefe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuzzy decision making: Measuring User Influence Based on Multiple Metrics on YouTube\n",
    "# TOPSIS; Fuzzy decision making origins: Fuzzy Multiple Attribute Decision Making Methods\n",
    "# TOPSIS Python: https://pypi.org/project/topsis-jamesfallon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0d113365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     min_views  max_views  avg_views  min_likes  max_likes  avg_likes  \\\n",
      "0     0.000150   0.001483   0.000873   0.000441   0.003265   0.001467   \n",
      "1     0.000350   0.000134   0.000343   0.000233   0.000140   0.000249   \n",
      "2     0.000503   0.000640   0.001216   0.000195   0.000368   0.000431   \n",
      "3     0.000490   0.000199   0.000501   0.000235   0.000286   0.000417   \n",
      "4     0.000362   0.000900   0.001397   0.000398   0.000880   0.001068   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "203   0.006276   0.001734   0.004999   0.000614   0.000383   0.000674   \n",
      "204   0.003092   0.001197   0.003062   0.001613   0.000605   0.001314   \n",
      "205   0.000867   0.001553   0.002987   0.000173   0.000448   0.000577   \n",
      "206   0.000692   0.001869   0.003068   0.000544   0.001978   0.002131   \n",
      "207   0.000773   0.000776   0.001379   0.000709   0.000981   0.001273   \n",
      "\n",
      "     min_comments  max_comments  avg_comments  \n",
      "0        0.001240      0.002716      0.002212  \n",
      "1        0.001420      0.000548      0.001264  \n",
      "2        0.001172      0.000457      0.001018  \n",
      "3        0.001804      0.000406      0.001183  \n",
      "4        0.001172      0.000949      0.001438  \n",
      "..            ...           ...           ...  \n",
      "203      0.001172      0.000447      0.001035  \n",
      "204      0.002886      0.001117      0.002573  \n",
      "205      0.001826      0.000711      0.001634  \n",
      "206      0.001375      0.002290      0.002787  \n",
      "207      0.001262      0.000909      0.001393  \n",
      "\n",
      "[208 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "numpy_array = merged_df[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "\n",
    "# Step 1: Calculate the normalized decision matrix\n",
    "\n",
    "# Square all values in column\n",
    "numpy_square = np.square(numpy_array)\n",
    "\n",
    "# Get the sum of squares and then take the squareroute of it\n",
    "sqrt_sum_0 = math.sqrt(np.sum(numpy_square[:, 0]))\n",
    "sqrt_sum_1 = math.sqrt(np.sum(numpy_square[:, 1]))\n",
    "sqrt_sum_2 = math.sqrt(np.sum(numpy_square[:, 2]))\n",
    "sqrt_sum_3 = math.sqrt(np.sum(numpy_square[:, 3]))\n",
    "sqrt_sum_4 = math.sqrt(np.sum(numpy_square[:, 4]))\n",
    "sqrt_sum_5 = math.sqrt(np.sum(numpy_square[:, 5]))\n",
    "sqrt_sum_6 = math.sqrt(np.sum(numpy_square[:, 6]))\n",
    "sqrt_sum_7 = math.sqrt(np.sum(numpy_square[:, 7]))\n",
    "sqrt_sum_8 = math.sqrt(np.sum(numpy_square[:, 8]))\n",
    "\n",
    "column_0 = numpy_array[:, 0]\n",
    "column_1 = numpy_array[:, 1]\n",
    "column_2 = numpy_array[:, 2]\n",
    "column_3 = numpy_array[:, 3]\n",
    "column_4 = numpy_array[:, 4]\n",
    "column_5 = numpy_array[:, 5]\n",
    "column_6 = numpy_array[:, 6]\n",
    "column_7 = numpy_array[:, 7]\n",
    "column_8 = numpy_array[:, 8]\n",
    "\n",
    "# Normalize by taking each row value and dividing it by the before calculated squareroot sum\n",
    "f = lambda x: x/sqrt_sum\n",
    "\n",
    "sqrt_sum = sqrt_sum_0\n",
    "nor_column_0 = f(column_0)\n",
    "sqrt_sum = sqrt_sum_1\n",
    "nor_column_1 = f(column_1)\n",
    "sqrt_sum = sqrt_sum_2\n",
    "nor_column_2 = f(column_2)\n",
    "sqrt_sum = sqrt_sum_3\n",
    "nor_column_3 = f(column_3)\n",
    "sqrt_sum = sqrt_sum_4\n",
    "nor_column_4 = f(column_4)\n",
    "sqrt_sum = sqrt_sum_5\n",
    "nor_column_5 = f(column_5)\n",
    "sqrt_sum = sqrt_sum_6\n",
    "nor_column_6 = f(column_6)\n",
    "sqrt_sum = sqrt_sum_7\n",
    "nor_column_7 = f(column_7)\n",
    "sqrt_sum = sqrt_sum_8\n",
    "nor_column_8 = f(column_8)\n",
    "\n",
    "# Step 2: Calculate the weighted decision matrix\n",
    "# Weight of 1/9 because of 9 attributes and Xiao etal. 2015: \"We suppose each metric has the same weight\"\n",
    "weight = 1/9\n",
    "\n",
    "weight_function = lambda x: x*weight\n",
    "weight_column_0 = weight_function(nor_column_0)\n",
    "weight_column_1 = weight_function(nor_column_1)\n",
    "weight_column_2 = weight_function(nor_column_2)\n",
    "weight_column_3 = weight_function(nor_column_3)\n",
    "weight_column_4 = weight_function(nor_column_4)\n",
    "weight_column_5 = weight_function(nor_column_5)\n",
    "weight_column_6 = weight_function(nor_column_6)\n",
    "weight_column_7 = weight_function(nor_column_7)\n",
    "weight_column_8 = weight_function(nor_column_8)\n",
    "\n",
    "# Step 3: Determine the ideal and negative-ideal solutions:\n",
    "max_column_0 = np.max(weight_column_0)\n",
    "max_column_1 = np.max(weight_column_1)\n",
    "max_column_2 = np.max(weight_column_2)\n",
    "max_column_3 = np.max(weight_column_3)\n",
    "max_column_4 = np.max(weight_column_4)\n",
    "max_column_5 = np.max(weight_column_5)\n",
    "max_column_6 = np.max(weight_column_6)\n",
    "max_column_7 = np.max(weight_column_7)\n",
    "max_column_8 = np.max(weight_column_8)\n",
    "\n",
    "min_column_0 = np.min(weight_column_0)\n",
    "min_column_1 = np.min(weight_column_1)\n",
    "min_column_2 = np.min(weight_column_2)\n",
    "min_column_3 = np.min(weight_column_3)\n",
    "min_column_4 = np.min(weight_column_4)\n",
    "min_column_5 = np.min(weight_column_5)\n",
    "min_column_6 = np.min(weight_column_6)\n",
    "min_column_7 = np.min(weight_column_7)\n",
    "min_column_8 = np.min(weight_column_8)\n",
    "\n",
    "ideal_max = (max_column_0, max_column_1, max_column_2, max_column_3, max_column_4, max_column_5, max_column_6, max_column_7, max_column_8)\n",
    "ideal_min = (min_column_0, min_column_1, min_column_2, min_column_3, min_column_4, min_column_5, min_column_6, min_column_7, min_column_8)\n",
    "\n",
    "# Step 4: Calculate the separation measures \n",
    "\n",
    "weighted_df = pd.DataFrame({'min_views': weight_column_0, 'max_views': weight_column_1, 'avg_views': weight_column_2, 'min_likes': weight_column_3, 'max_likes': weight_column_4, 'avg_likes': weight_column_5, 'min_comments': weight_column_6, 'max_comments': weight_column_7, 'avg_comments': weight_column_8})\n",
    "print(weighted_df)\n",
    "\n",
    "#weighted_array = weighted_df[[\"min_views\", \"max_views\", \"avg_views\", \"min_likes\", \"max_likes\", \"avg_likes\", \"min_comments\", \"max_comments\", \"avg_comments\"]].to_numpy()\n",
    "\n",
    "#print(weighted_array)\n",
    "\n",
    "#euc_pos = lambda x: distance.euclidean(ideal_max, x)\n",
    "\n",
    "#ideal_pos_array = euc_pos(weighted_array)\n",
    "\n",
    "#print(ideal_pos_array)\n",
    "def euclidian_max(a, b, c, d, e, f, g, h, i):\n",
    "    row_vector = (a, b, c, d, e, f, g, h, i)\n",
    "    return distance.euclidean(row_vector, ideal_max)\n",
    "\n",
    "def euclidian_min(a, b, c, d, e, f, g, h, i):\n",
    "    row_vector = (a, b, c, d, e, f, g, h, i)\n",
    "    return distance.euclidean(row_vector, ideal_min)\n",
    "\n",
    "\n",
    "weighted_df['euc_pos'] = weighted_df.apply(lambda row : euclidian_max(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "weighted_df['euc_neg'] = weighted_df.apply(lambda row : euclidian_min(row['min_views'],\n",
    "                     row['max_views'], row['avg_views'], row['min_likes'], row['max_likes'], row['avg_likes'], row['min_comments'], row['max_comments'], row['avg_comments']), axis = 1)\n",
    "\n",
    "# Step 5: Calculate the relative closeness to the ideal solution:\n",
    "def relative_closeness(pos, neg):\n",
    "    relative = neg/(pos-neg)\n",
    "    \n",
    "    return relative\n",
    "\n",
    "weighted_df['euc_rel'] = weighted_df.apply(lambda row : relative_closeness(row['euc_pos'],\n",
    "                     row['euc_neg']), axis = 1)\n",
    "\n",
    "\n",
    "# Step 6: Rank the preference order:\n",
    "channel_id = merged_df['channel_id'].tolist()\n",
    "channel_title = merged_df['channel_title'].tolist()\n",
    "weighted_df['channel_id'] = channel_id\n",
    "weighted_df['channel_title'] = channel_title\n",
    "weighted_df = weighted_df.sort_values(by='euc_rel', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "66f81736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     min_views  max_views  avg_views  min_likes  max_likes  avg_likes  \\\n",
      "170   0.040320   0.011784   0.033237   0.068292   0.026638   0.056771   \n",
      "165   0.033074   0.008312   0.024895   0.006854   0.002930   0.005990   \n",
      "123   0.016285   0.010619   0.021255   0.019535   0.016394   0.024889   \n",
      "166   0.023594   0.005109   0.016326   0.030903   0.016309   0.030537   \n",
      "145   0.002946   0.014349   0.010890   0.003953   0.023182   0.012049   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "150   0.000191   0.000035   0.000120   0.000248   0.000119   0.000232   \n",
      "134   0.000124   0.000098   0.000214   0.000163   0.000117   0.000206   \n",
      "24    0.000165   0.000117   0.000257   0.000085   0.000122   0.000172   \n",
      "22    0.078219   0.022048   0.063548   0.064990   0.026532   0.054659   \n",
      "90    0.000368   0.091828   0.036844   0.000150   0.080605   0.021075   \n",
      "\n",
      "     min_comments  max_comments  avg_comments   euc_pos   euc_neg    euc_rel  \\\n",
      "170      0.053903      0.012276      0.035556  0.130301  0.124792  22.653728   \n",
      "165      0.048988      0.012687      0.034543  0.167848  0.073792   0.784559   \n",
      "123      0.021124      0.020733      0.037183  0.157935  0.065015   0.699683   \n",
      "166      0.024934      0.011742      0.025279  0.159318  0.065459   0.697418   \n",
      "145      0.003021      0.033435      0.017116  0.172241  0.048914   0.396622   \n",
      "..            ...           ...           ...       ...       ...        ...   \n",
      "150      0.001240      0.000289      0.000828  0.210604  0.000330   0.001568   \n",
      "134      0.001150      0.000426      0.000961  0.210551  0.000328   0.001563   \n",
      "24       0.001172      0.000315      0.000843  0.210609  0.000286   0.001359   \n",
      "22       0.049078      0.016662      0.040502  0.111772  0.150551  -3.882251   \n",
      "90       0.001172      0.084934      0.030468  0.124790  0.157337  -4.834071   \n",
      "\n",
      "                   channel_id                         channel_title  \n",
      "170  UCzH549YlZhdhIqhtvz7XHmQ                             AlexiBexi  \n",
      "165  UCn7wWR5KnpX_N6ZaBNuyVYw                                   WDR  \n",
      "123  UCcnmNzv0yEEYPM6Oa86unhQ  Die Autodoktoren - offizieller Kanal  \n",
      "166  UCkeQaTKHKmRnWmXnl0uIuSA                          Maeximiliano  \n",
      "145  UCiZnK4X73okItJqaCs2YDOQ                            Car Maniac  \n",
      "..                        ...                                   ...  \n",
      "150  UCfs-nDj_nyekl_M-3SkLNfA                             Ecofahrer  \n",
      "134  UCnEEC46gghhncsBjuxgxEvw                           E.lektrisch  \n",
      "24   UC-r2l4o0_R3eRwOHthmGAcA                            Tesla Kägi  \n",
      "22   UC1-VOKyTJrgLiBeiJqzeIUQ                        JP Performance  \n",
      "90   UCcweJsCV2TUP_kzMX25U-oQ                                  BR24  \n",
      "\n",
      "[208 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(weighted_df)\n",
    "weighted_df.to_csv('influence_euc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ac710015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   channel_id                 channel_title  \\\n",
      "0    UCBCT7EV5YrNLPvrhXYTHJuw       Jürs Lackiererei Lübeck   \n",
      "1    UCAszOEwa5CS4WFwYpkjdaUQ                  heise online   \n",
      "2    UCBTJnSirNI8kwOn5ROB_BGA            Autozentrum Walter   \n",
      "3    UC81R4gy-4qqoV6l6OUpDeIg                      Techning   \n",
      "4    UC96MGGyxaZZQ6aFKSVMHIew                   ECARIO.info   \n",
      "..                        ...                           ...   \n",
      "203  UCvKSljIz5MuYz0J7GnpvPHQ                   AutoScout24   \n",
      "204  UCy82YSU9FxoYhknZ4PFmjrQ                    E3/DC GmbH   \n",
      "205  UCvwE_EBBDWM552Vd9v8m9Gg  MDR Mitteldeutscher Rundfunk   \n",
      "206  UCutQkZpAs9xG0KaooZlnuTw            Motor1 Deutschland   \n",
      "207  UCwFUwxY9TUiVwr4vQ3zX_6w                    MotorWoche   \n",
      "\n",
      "                                   channel_description channel_subscribers  \\\n",
      "0    Mein Name ist Michael Scharnberg, ich bin Fahr...               14400   \n",
      "1    Unser wöchentlicher Veröffentlichungsplan:\\nMo...               82500   \n",
      "2    Unser Autozentrum Walter aus Pforzheim / Enzkr...                4380   \n",
      "3    Hi, mein Name ist Henning und ich interessiere...                 523   \n",
      "4    Bei ECARIO dreht sich alles um die Themen E-Mo...               13500   \n",
      "..                                                 ...                 ...   \n",
      "203  Wir vergleichen, testen, interviewen und stell...               24600   \n",
      "204  100% Einsatz für 100% Autarkie\\n\\nEnergie selb...               18400   \n",
      "205  Willkommen im offiziellen YouTube-Kanal des Mi...              190000   \n",
      "206  Wir machen was mit und über Autos… wie so viel...               62200   \n",
      "207  Automotive\\n\\nImpressum\\nAngaben gemäß §5 TMG ...              124000   \n",
      "\n",
      "                                         channel_topic  view_count  \\\n",
      "0    [https://en.wikipedia.org/wiki/Lifestyle_(soci...      158461   \n",
      "1              [https://en.wikipedia.org/wiki/Society]       10390   \n",
      "2    [https://en.wikipedia.org/wiki/Lifestyle_(soci...      110458   \n",
      "3    [https://en.wikipedia.org/wiki/Lifestyle_(soci...       15173   \n",
      "4    [https://en.wikipedia.org/wiki/Lifestyle_(soci...      697975   \n",
      "..                                                 ...         ...   \n",
      "203  [https://en.wikipedia.org/wiki/Vehicle, https:...      151306   \n",
      "204            [https://en.wikipedia.org/wiki/Society]       92696   \n",
      "205  [https://en.wikipedia.org/wiki/Lifestyle_(soci...       90424   \n",
      "206  [https://en.wikipedia.org/wiki/Lifestyle_(soci...      371509   \n",
      "207  [https://en.wikipedia.org/wiki/Vehicle, https:...      187881   \n",
      "\n",
      "     like_count  comment_count  min_views  max_views     avg_views  min_likes  \\\n",
      "0          9080           1795       1425      78479  13205.083333        176   \n",
      "1           257            171       3320       7070   5195.000000         93   \n",
      "2          1334            413       4765      33897  18409.666667         78   \n",
      "3           430            160       4644      10529   7586.500000         94   \n",
      "4         18178           3210       3428      47639  21150.757576        159   \n",
      "..          ...            ...        ...        ...           ...        ...   \n",
      "203         695            140      59504      91802  75653.000000        245   \n",
      "204        1355            348      29319      63377  46348.000000        644   \n",
      "205         595            221       8216      82208  45212.000000         69   \n",
      "206        8793           1508       6563      98963  46438.625000        217   \n",
      "207        5907            848       7328      41087  20875.666667        283   \n",
      "\n",
      "     max_likes    avg_likes  min_comments  max_comments  avg_comments  \n",
      "0         3836   756.666667            55           535    149.583333  \n",
      "1          164   128.500000            63           108     85.500000  \n",
      "2          432   222.333333            52            90     68.833333  \n",
      "3          336   215.000000            80            80     80.000000  \n",
      "4         1034   550.848485            52           187     97.272727  \n",
      "..         ...          ...           ...           ...           ...  \n",
      "203        450   347.500000            52            88     70.000000  \n",
      "204        711   677.500000           128           220    174.000000  \n",
      "205        526   297.500000            81           140    110.500000  \n",
      "206       2324  1099.125000            61           451    188.500000  \n",
      "207       1153   656.333333            56           179     94.222222  \n",
      "\n",
      "[208 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "20e7cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2.   1500.  20000.      5.5     5.      9. ]\n",
      " [    2.5  2700.  18000.      6.5     3.      5. ]\n",
      " [    1.8  2000.  21000.      4.5     7.      7. ]\n",
      " [    2.2  1800.  20000.      5.      5.      5. ]]\n",
      "4\n",
      "   Column1  Column2  Column3  Column4\n",
      "0      2.0   1500.0  20000.0      5.5\n",
      "1      2.5   2700.0  18000.0      6.5\n",
      "2      1.8   2000.0  21000.0      4.5\n",
      "3      2.2   1800.0  20000.0      5.0\n"
     ]
    }
   ],
   "source": [
    "test_array = np.array([[2.0, 1500.0, 20000.0, 5.5, 5.0, 9.0], [2.5, 2700.0, 18000.0, 6.5, 3.0, 5.0], [1.8, 2000.0, 21000.0, 4.5, 7.0, 7.0], [2.2, 1800.0, 20000.0, 5.0, 5.0, 5.0]])\n",
    "print(test_array)\n",
    "\n",
    "test_array_square = np.square(test_array)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "test_sum_0 = math.sqrt(np.sum(test_array_square[:, 0]))\n",
    "test_sum_1 = math.sqrt(np.sum(test_array_square[:, 1]))\n",
    "test_sum_2 = math.sqrt(np.sum(test_array_square[:, 2]))\n",
    "test_sum_3 = math.sqrt(np.sum(test_array_square[:, 3]))\n",
    "test_sum_4 = math.sqrt(np.sum(test_array_square[:, 4]))\n",
    "\n",
    "\n",
    "column_0 = test_array[:, 0]\n",
    "column_1 = test_array[:, 1]\n",
    "column_2 = test_array[:, 2]\n",
    "column_3 = test_array[:, 3]\n",
    "\n",
    "f = lambda x: x/test_sum_0\n",
    "normalize_0 = f(column_0)\n",
    "\n",
    "print(len(column_3))\n",
    "\n",
    "dataset = pd.DataFrame({'Column1': column_0, 'Column2': column_1, 'Column3': column_2, 'Column4': column_3})\n",
    "\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "20169c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.196152422706632\n"
     ]
    }
   ],
   "source": [
    "a = (1, 2, 3)\n",
    "b = (4, 5, 6)\n",
    "dst = distance.euclidean(a, b)\n",
    "print(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e87efc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2.   1500.  20000.      5.5     5.      9. ]\n",
      " [    2.5  2700.  18000.      6.5     3.      5. ]\n",
      " [    1.8  2000.  21000.      4.5     7.      7. ]\n",
      " [    2.2  1800.  20000.      5.      5.      5. ]]\n"
     ]
    }
   ],
   "source": [
    "print(test_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
